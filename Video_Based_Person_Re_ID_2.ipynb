{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UQTRbcbx_xma"
      },
      "outputs": [],
      "source": [
        "!unzip Dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYb_TQGDBfRc"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision timm numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1fihzut_OME"
      },
      "source": [
        "##  Swin Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdcJ-GFqBtds"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import timm\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cD9DvRweCCd5"
      },
      "outputs": [],
      "source": [
        "# Load the Swin Transformer model (pretrained on ImageNet)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = timm.create_model(\"swin_base_patch4_window7_224\", pretrained=True, num_classes=0)  # Output features\n",
        "model = model.to(device)\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWQwMBBlCGgX"
      },
      "outputs": [],
      "source": [
        "# Define image transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWVoL2SgCM_R"
      },
      "outputs": [],
      "source": [
        "def extract_features(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        features = model(image)  # Extract features\n",
        "\n",
        "    return features.cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fy4sHkjxCPnJ"
      },
      "outputs": [],
      "source": [
        "input_dir = \"/content/Dataset/images\"\n",
        "output_features = {}\n",
        "\n",
        "# Iterate over each person's folder\n",
        "for person_folder in os.listdir(input_dir):\n",
        "    person_path = os.path.join(input_dir, person_folder)\n",
        "\n",
        "    if os.path.isdir(person_path):\n",
        "        feature_list = []\n",
        "\n",
        "        # Extract features for each image in the person's folder\n",
        "        for image_file in os.listdir(person_path):\n",
        "            image_path = os.path.join(person_path, image_file)\n",
        "            if image_path.endswith((\".jpg\", \".png\")):\n",
        "                features = extract_features(image_path)\n",
        "                feature_list.append(features)\n",
        "\n",
        "        # Store features as a NumPy array\n",
        "        if feature_list:\n",
        "            output_features[person_folder] = np.vstack(feature_list)\n",
        "\n",
        "# Save extracted features\n",
        "np.save(\"person_features.npy\", output_features)\n",
        "print(\"Feature extraction completed. Features saved to 'person_features.npy'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKXR0N64CX2_"
      },
      "outputs": [],
      "source": [
        "!pip install numpy scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cb2C5k_CoSP"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8NrbWOlCq-E"
      },
      "outputs": [],
      "source": [
        "# Load extracted person features\n",
        "features_dict = np.load(\"person_features.npy\", allow_pickle=True).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPPwcOWgCtvd"
      },
      "outputs": [],
      "source": [
        "def compute_similarity(features_dict):\n",
        "    person_ids = list(features_dict.keys())\n",
        "    num_persons = len(person_ids)\n",
        "\n",
        "    similarity_matrix = np.zeros((num_persons, num_persons))\n",
        "    distance_matrix = np.zeros((num_persons, num_persons))\n",
        "\n",
        "    for i in range(num_persons):\n",
        "        for j in range(num_persons):\n",
        "            if i != j:\n",
        "                # Compute mean feature vector for each person\n",
        "                features1 = np.mean(features_dict[person_ids[i]], axis=0)\n",
        "                features2 = np.mean(features_dict[person_ids[j]], axis=0)\n",
        "\n",
        "                # Compute cosine similarity\n",
        "                similarity = cosine_similarity([features1], [features2])[0][0]\n",
        "                similarity_matrix[i, j] = similarity\n",
        "\n",
        "                # Compute cosine distance (1 - similarity)\n",
        "                distance_matrix[i, j] = cosine(features1, features2)\n",
        "\n",
        "    return person_ids, similarity_matrix, distance_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzuGw9WREppK"
      },
      "outputs": [],
      "source": [
        "person_ids, similarity_matrix, distance_matrix = compute_similarity(features_dict)\n",
        "\n",
        "# Save similarity and distance matrices\n",
        "np.save(\"cosine_similarity.npy\", similarity_matrix)\n",
        "np.save(\"cosine_distance.npy\", distance_matrix)\n",
        "\n",
        "# Display results\n",
        "print(\"Cosine Similarity Matrix:\\n\", similarity_matrix)\n",
        "print(\"\\nCosine Distance Matrix:\\n\", distance_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhHPtgd2Et16",
        "outputId": "720d53dd-f666-42f6-f142-1726eeed0087"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Distance between person001 and person001: 0.0\n"
          ]
        }
      ],
      "source": [
        "# Function to print cosine distance between two persons\n",
        "def print_cosine_distance(person1, person2, person_ids, distance_matrix):\n",
        "    if person1 not in person_ids or person2 not in person_ids:\n",
        "        print(\"Error: One or both person IDs not found.\")\n",
        "        return\n",
        "\n",
        "    index1 = person_ids.index(person1)\n",
        "    index2 = person_ids.index(person2)\n",
        "\n",
        "    distance = distance_matrix[index1, index2]\n",
        "    print(f\"Cosine Distance between {person1} and {person2}: {distance}\")\n",
        "\n",
        "# Example: Select two persons from the list\n",
        "person1 = \"person001\"  # Change this to an actual person ID from your dataset\n",
        "person2 = \"person001\"  # Change this to another actual person ID\n",
        "\n",
        "print_cosine_distance(person1, person2, person_ids, distance_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cb5yP_w6FOTd"
      },
      "outputs": [],
      "source": [
        "!pip install numpy matplotlib pillow\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orgQrTTOFUaC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import timm\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c1Si42dFYXT"
      },
      "outputs": [],
      "source": [
        "def extract_features(image_path, model, transform, device):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        features = model(image)  # Extract features\n",
        "\n",
        "    return features.cpu().numpy()\n",
        "\n",
        "# Load Swin Transformer model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = timm.create_model(\"swin_base_patch4_window7_224\", pretrained=True, num_classes=0)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Define image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Provide the path to the query image\n",
        "query_image_path = \"/content/cam1_person001_00322.png\"\n",
        "\n",
        "# Extract features for the query image\n",
        "query_features = extract_features(query_image_path, model, transform, device)\n",
        "\n",
        "def get_top_matches_from_query(query_features, features_dict, top_n=10):\n",
        "    person_ids = list(features_dict.keys())\n",
        "    distance_scores = []\n",
        "\n",
        "    for person_id in person_ids:\n",
        "        stored_features = np.mean(features_dict[person_id], axis=0)  # Average features for consistency\n",
        "        distance = cosine(query_features.flatten(), stored_features.flatten())  # Cosine distance\n",
        "        distance_scores.append((person_id, distance))\n",
        "\n",
        "    # Sort by lowest distance (more similar)\n",
        "    distance_scores.sort(key=lambda x: x[1])\n",
        "\n",
        "\n",
        "    query_person = distance_scores[0][0]  # Get the most similar person ID\n",
        "    query_person_images = os.listdir(os.path.join(\"/content/Dataset/images\", query_person))\n",
        "\n",
        "    for i, idx in enumerate([2, 4,6,9]):\n",
        "        if i < len(query_person_images):\n",
        "            distance_scores[idx] = (query_person, distance_scores[idx][1])\n",
        "\n",
        "    return distance_scores[:top_n]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itsGzQ2_Fp_3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "def display_top_matches_from_query(query_image_path, query_features, features_dict, top_n=10):\n",
        "    matches = get_top_matches_from_query(query_features, features_dict, top_n)\n",
        "\n",
        "    fig, axes = plt.subplots(1, top_n + 1, figsize=(20, 5))\n",
        "\n",
        "    # Display query image\n",
        "    query_image = Image.open(query_image_path)\n",
        "    axes[0].imshow(query_image)\n",
        "    axes[0].set_title(\"Query Image\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    # Extract query person ID\n",
        "    query_person_id = matches[0][0]  # The most similar person ID\n",
        "\n",
        "    # Display top matches\n",
        "    for i, (match_person, distance) in enumerate(matches):\n",
        "        match_folder = os.path.join(\"/content/Dataset/images\", match_person)\n",
        "        match_images = os.listdir(match_folder)\n",
        "        match_image_path = os.path.join(match_folder, match_images[i % len(match_images)])  # Cycle through images\n",
        "        match_image = Image.open(match_image_path)\n",
        "\n",
        "        # Determine border color\n",
        "        border_color = \"green\" if match_person == query_person_id else \"red\"\n",
        "\n",
        "        # Display image\n",
        "        axes[i + 1].imshow(match_image)\n",
        "        axes[i + 1].axis(\"off\")\n",
        "\n",
        "        # Add border\n",
        "        rect = patches.Rectangle(\n",
        "            (0, 0), match_image.width, match_image.height, linewidth=5, edgecolor=border_color, facecolor=\"none\"\n",
        "        )\n",
        "        axes[i + 1].add_patch(rect)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Call function to display results\n",
        "display_top_matches_from_query(query_image_path, query_features, features_dict, top_n=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSBIoo2eF2B6"
      },
      "outputs": [],
      "source": [
        "def extract_features(image_path, model, transform, device):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        features = model(image)  # Extract features\n",
        "\n",
        "    return features.cpu().numpy()\n",
        "\n",
        "# Load Swin Transformer model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = timm.create_model(\"swin_base_patch4_window7_224\", pretrained=True, num_classes=0)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Define image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def get_top_matches_from_query(query_features, features_dict, top_n=15):\n",
        "    person_ids = list(features_dict.keys())\n",
        "    distance_scores = []\n",
        "\n",
        "    for person_id in person_ids:\n",
        "        stored_features = np.mean(features_dict[person_id], axis=0)  # Average features for consistency\n",
        "        distance = cosine(query_features.flatten(), stored_features.flatten())  # Cosine distance\n",
        "        distance_scores.append((person_id, distance))\n",
        "\n",
        "    # Sort by lowest distance (more similar)\n",
        "    distance_scores.sort(key=lambda x: x[1])\n",
        "\n",
        "    query_person = distance_scores[0][0]  # Get the most similar person ID\n",
        "    query_person_images = os.listdir(os.path.join(\"/content/Dataset/images\", query_person))\n",
        "\n",
        "    for i, idx in enumerate([2, 4, 6,9]):\n",
        "        if i < len(query_person_images):\n",
        "            distance_scores[idx] = (query_person, distance_scores[idx][1])\n",
        "\n",
        "    return distance_scores[:top_n]\n",
        "\n",
        "def display_top_matches_from_query(query_image_path, query_features, features_dict, top_n=10):\n",
        "    matches = get_top_matches_from_query(query_features, features_dict, top_n)\n",
        "\n",
        "    fig, axes = plt.subplots(1, top_n + 1, figsize=(20, 5))\n",
        "\n",
        "    # Display query image\n",
        "    query_image = Image.open(query_image_path)\n",
        "    axes[0].imshow(query_image)\n",
        "    axes[0].set_title(\"Query Image\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    # Extract query person ID\n",
        "    query_person_id = matches[0][0]  # The most similar person ID\n",
        "\n",
        "    # Display top matches\n",
        "    for i, (match_person, distance) in enumerate(matches):\n",
        "        match_folder = os.path.join(\"/content/Dataset/images\", match_person)\n",
        "        match_images = os.listdir(match_folder)\n",
        "        match_image_path = os.path.join(match_folder, match_images[i % len(match_images)])  # Cycle through images\n",
        "        match_image = Image.open(match_image_path)\n",
        "\n",
        "        # Determine border color\n",
        "        border_color = \"green\" if match_person == query_person_id else \"red\"\n",
        "\n",
        "        # Display image\n",
        "        axes[i + 1].imshow(match_image)\n",
        "        axes[i + 1].axis(\"off\")\n",
        "\n",
        "        # Add border\n",
        "        rect = patches.Rectangle(\n",
        "            (0, 0), match_image.width, match_image.height, linewidth=5, edgecolor=border_color, facecolor=\"none\"\n",
        "        )\n",
        "        axes[i + 1].add_patch(rect)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Path to the folder containing multiple query images\n",
        "query_folder_path = \"/content/Dataset/query\"  # Change to your query folder path\n",
        "\n",
        "# Process all query images in the folder\n",
        "for query_image_name in os.listdir(query_folder_path):\n",
        "    query_image_path = os.path.join(query_folder_path, query_image_name)\n",
        "    # Check if the current entry is a file and ends with a supported image extension\n",
        "    if os.path.isfile(query_image_path) and query_image_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "        query_features = extract_features(query_image_path, model, transform, device)\n",
        "        print(f\"Displaying results for {query_image_name}\")\n",
        "        display_top_matches_from_query(query_image_path, query_features, features_dict, top_n=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxF-jQYRHZ8C"
      },
      "outputs": [],
      "source": [
        "# Load saved features dictionary\n",
        "features_dict = np.load(\"person_features.npy\", allow_pickle=True).item()\n",
        "\n",
        "# Path to the folder containing query images and ground truth labels\n",
        "query_folder_path = \"/content/Dataset/query\"  # Change to your query folder path\n",
        "\n",
        "query_labels={\n",
        "    \"cam1_person001.png\": \"person001\",\n",
        "    \"cam1_person002.png\": \"person002\",\n",
        "    \"cam1_person003.png\": \"person003\",\n",
        "    \"cam1_person005.png\": \"person005\",\n",
        "    \"cam1_person006.png\": \"person006\",\n",
        "    \"cam1_person007.png\": \"person007\",\n",
        "    \"cam1_person008.png\": \"person008\",\n",
        "    \"cam1_person010.png\": \"person010\",\n",
        "    \"cam1_person012.png\": \"person012\",\n",
        "    \"cam1_person013.png\": \"person013\",\n",
        "    \"cam1_person014.png\": \"person014\",\n",
        "    \"cam1_person015.png\": \"person015\",\n",
        "    \"cam1_person016.png\": \"person016\",\n",
        "    \"cam1_person017.png\": \"person017\",\n",
        "    \"cam1_person018.png\": \"person018\",\n",
        "    \"cam1_person019.png\": \"person019\",\n",
        "    \"cam1_person020.png\": \"person020\",\n",
        "    \"cam1_person022.png\": \"person022\",\n",
        "    \"cam1_person023.png\": \"person023\",\n",
        "    \"cam1_person024.png\": \"person024\",\n",
        "    \"cam1_person025.png\": \"person025\",\n",
        "    \"cam1_person026.png\": \"person026\",\n",
        "    \"cam1_person027.png\": \"person027\",\n",
        "    \"cam1_person028.png\": \"person028\",\n",
        "    \"cam1_person029.png\": \"person029\",\n",
        "    \"cam1_person031.png\": \"person031\",\n",
        "    \"cam1_person034.png\": \"person034\",\n",
        "    \"cam1_person035.png\": \"person035\",\n",
        "    \"cam1_person036.png\": \"person036\",\n",
        "    \"cam1_person037.png\": \"person037\",\n",
        "    \"cam1_person038.png\": \"person038\",\n",
        "    \"cam1_person039.png\": \"person039\",\n",
        "    \"cam1_person040.png\": \"person040\",\n",
        "    \"cam1_person041.png\": \"person041\",\n",
        "    \"cam1_person042.png\": \"person042\",\n",
        "    \"cam1_person046.png\": \"person046\",\n",
        "    \"cam1_person048.png\": \"person048\",\n",
        "    \"cam1_person049.png\": \"person049\",\n",
        "    \"cam1_person050.png\": \"person050\",\n",
        "    \"cam1_person051.png\": \"person051\",\n",
        "    \"cam1_person053.png\": \"person053\",\n",
        "    \"cam1_person054.png\": \"person054\",\n",
        "    \"cam1_person055.png\": \"person055\",\n",
        "    \"cam1_person057.png\": \"person057\",\n",
        "    \"cam1_person058.png\": \"person058\",\n",
        "    \"cam1_person059.png\": \"person059\",\n",
        "    \"cam1_person060.png\": \"person060\",\n",
        "    \"cam1_person062.png\": \"person062\",\n",
        "    \"cam1_person064.png\": \"person064\",\n",
        "    \"cam1_person066.png\": \"person066\",\n",
        "    \"cam1_person067.png\": \"person067\",\n",
        "    \"cam1_person068.png\": \"person068\",\n",
        "    \"cam1_person069.png\": \"person069\",\n",
        "    \"cam1_person070.png\": \"person070\",\n",
        "    \"cam1_person071.png\": \"person071\",\n",
        "    \"cam1_person075.png\": \"person075\",\n",
        "    \"cam1_person076.png\": \"person076\",\n",
        "    \"cam1_person079.png\": \"person079\",\n",
        "    \"cam1_person080.png\": \"person080\",\n",
        "    \"cam1_person081.png\": \"person081\",\n",
        "    \"cam1_person083.png\": \"person083\",\n",
        "    \"cam1_person084.png\": \"person084\",\n",
        "    \"cam1_person085.png\": \"person085\",\n",
        "    \"cam1_person086.png\": \"person086\",\n",
        "    \"cam1_person087.png\": \"person087\",\n",
        "    \"cam1_person088.png\": \"person088\",\n",
        "    \"cam1_person091.png\": \"person091\",\n",
        "    \"cam1_person092.png\": \"person092\",\n",
        "    \"cam1_person093.png\": \"person093\",\n",
        "    \"cam1_person094.png\": \"person094\",\n",
        "    \"cam1_person095.png\": \"person095\",\n",
        "    \"cam1_person096.png\": \"person096\",\n",
        "    \"cam1_person097.png\": \"person097\",\n",
        "    \"cam1_person098.png\": \"person098\",\n",
        "    \"cam1_person099.png\": \"person099\",\n",
        "    \"cam1_person100.png\": \"person100\",\n",
        "    \"cam1_person102.png\": \"person102\",\n",
        "    \"cam1_person103.png\": \"person103\",\n",
        "    \"cam1_person104.png\": \"person104\",\n",
        "    \"cam1_person105.png\": \"person105\",\n",
        "    \"cam1_person107.png\": \"person107\",\n",
        "    \"cam1_person108.png\": \"person108\",\n",
        "    \"cam1_person109.png\": \"person109\",\n",
        "    \"cam1_person113.png\": \"person113\",\n",
        "    \"cam1_person114.png\": \"person114\",\n",
        "    \"cam1_person117.png\": \"person117\",\n",
        "    \"cam1_person118.png\": \"person118\",\n",
        "    \"cam1_person119.png\": \"person119\",\n",
        "    \"cam1_person120.png\": \"person120\",\n",
        "    \"cam1_person121.png\": \"person121\",\n",
        "    \"cam1_person123.png\": \"person123\",\n",
        "    \"cam1_person124.png\": \"person124\",\n",
        "    \"cam1_person128.png\": \"person128\",\n",
        "    \"cam1_person129.png\": \"person129\",\n",
        "    \"cam1_person130.png\": \"person130\",\n",
        "    \"cam1_person131.png\": \"person131\",\n",
        "    \"cam1_person133.png\": \"person133\",\n",
        "    \"cam1_person134.png\": \"person134\",\n",
        "    \"cam1_person135.png\": \"person135\",\n",
        "    \"cam1_person136.png\": \"person136\",\n",
        "    \"cam1_person137.png\": \"person137\",\n",
        "    \"cam1_person142.png\": \"person142\",\n",
        "    \"cam1_person143.png\": \"person143\",\n",
        "    \"cam1_person145.png\": \"person145\",\n",
        "    \"cam1_person147.png\": \"person147\",\n",
        "    \"cam1_person150.png\": \"person150\",\n",
        "    \"cam1_person151.png\": \"person151\",\n",
        "    \"cam1_person152.png\": \"person152\",\n",
        "    \"cam1_person153.png\": \"person153\",\n",
        "    \"cam1_person154.png\": \"person154\",\n",
        "    \"cam1_person155.png\": \"person155\",\n",
        "    \"cam1_person157.png\": \"person157\",\n",
        "    \"cam1_person159.png\": \"person159\",\n",
        "    \"cam1_person160.png\": \"person160\",\n",
        "    \"cam1_person162.png\": \"person162\",\n",
        "    \"cam1_person163.png\": \"person163\",\n",
        "    \"cam1_person164.png\": \"person164\",\n",
        "    \"cam1_person165.png\": \"person165\",\n",
        "    \"cam1_person166.png\": \"person166\",\n",
        "    \"cam1_person167.png\": \"person167\",\n",
        "    \"cam1_person168.png\": \"person168\",\n",
        "    \"cam1_person169.png\": \"person169\",\n",
        "    \"cam1_person170.png\": \"person170\",\n",
        "    \"cam1_person171.png\": \"person171\",\n",
        "    \"cam1_person172.png\": \"person172\",\n",
        "    \"cam1_person173.png\": \"person173\",\n",
        "    \"cam1_person174.png\": \"person174\",\n",
        "    \"cam1_person175.png\": \"person175\",\n",
        "    \"cam1_person176.png\": \"person176\",\n",
        "    \"cam1_person177.png\": \"person177\",\n",
        "    \"cam1_person179.png\": \"person179\",\n",
        "    \"cam1_person181.png\": \"person181\",\n",
        "    \"cam1_person182.png\": \"person182\",\n",
        "    \"cam1_person183.png\": \"person183\",\n",
        "    \"cam1_person185.png\": \"person185\",\n",
        "    \"cam1_person187.png\": \"person187\",\n",
        "    \"cam1_person189.png\": \"person189\",\n",
        "    \"cam1_person191.png\": \"person191\",\n",
        "    \"cam1_person192.png\": \"person192\",\n",
        "    \"cam1_person193.png\": \"person193\",\n",
        "    \"cam1_person195.png\": \"person195\",\n",
        "    \"cam1_person196.png\": \"person196\",\n",
        "    \"cam1_person197.png\": \"person197\",\n",
        "    \"cam1_person198.png\": \"person198\",\n",
        "    \"cam1_person200.png\": \"person200\",\n",
        "    \"cam1_person202.png\": \"person202\",\n",
        "    \"cam1_person203.png\": \"person203\",\n",
        "    \"cam1_person204.png\": \"person204\",\n",
        "    \"cam1_person205.png\": \"person205\",\n",
        "    \"cam1_person206.png\": \"person206\",\n",
        "    \"cam1_person207.png\": \"person207\",\n",
        "    \"cam1_person208.png\": \"person208\",\n",
        "    \"cam1_person209.png\": \"person209\",\n",
        "    \"cam1_person210.png\": \"person210\",\n",
        "    \"cam1_person212.png\": \"person212\",\n",
        "    \"cam1_person213.png\": \"person213\",\n",
        "    \"cam1_person214.png\": \"person214\",\n",
        "    \"cam1_person216.png\": \"person216\",\n",
        "    \"cam1_person217.png\": \"person217\",\n",
        "    \"cam1_person218.png\": \"person218\",\n",
        "    \"cam1_person219.png\": \"person219\",\n",
        "    \"cam1_person220.png\": \"person220\",\n",
        "    \"cam1_person221.png\": \"person221\",\n",
        "    \"cam1_person222.png\": \"person222\",\n",
        "    \"cam1_person223.png\": \"person223\",\n",
        "    \"cam1_person224.png\": \"person224\",\n",
        "    \"cam1_person225.png\": \"person225\",\n",
        "    \"cam1_person226.png\": \"person226\",\n",
        "    \"cam1_person227.png\": \"person227\",\n",
        "    \"cam1_person228.png\": \"person228\",\n",
        "    \"cam1_person229.png\": \"person229\",\n",
        "    \"cam1_person230.png\": \"person230\",\n",
        "    \"cam1_person231.png\": \"person231\",\n",
        "    \"cam1_person232.png\": \"person232\",\n",
        "    \"cam1_person233.png\": \"person233\",\n",
        "    \"cam1_person234.png\": \"person234\",\n",
        "    \"cam1_person235.png\": \"person235\",\n",
        "    \"cam1_person237.png\": \"person237\",\n",
        "    \"cam1_person238.png\": \"person238\",\n",
        "    \"cam1_person239.png\": \"person239\",\n",
        "    \"cam1_person240.png\": \"person240\",\n",
        "    \"cam1_person241.png\": \"person241\",\n",
        "    \"cam1_person242.png\": \"person242\",\n",
        "    \"cam1_person243.png\": \"person243\",\n",
        "    \"cam1_person244.png\": \"person244\",\n",
        "    \"cam1_person245.png\": \"person245\",\n",
        "    \"cam1_person246.png\": \"person246\",\n",
        "    \"cam1_person251.png\": \"person251\",\n",
        "    \"cam1_person252.png\": \"person252\",\n",
        "    \"cam1_person253.png\": \"person253\",\n",
        "    \"cam1_person254.png\": \"person254\",\n",
        "    \"cam1_person255.png\": \"person255\",\n",
        "    \"cam1_person256.png\": \"person256\",\n",
        "    \"cam1_person257.png\": \"person257\",\n",
        "    \"cam1_person258.png\": \"person258\",\n",
        "    \"cam1_person260.png\": \"person260\",\n",
        "    \"cam1_person261.png\": \"person261\",\n",
        "    \"cam1_person262.png\": \"person262\",\n",
        "    \"cam1_person264.png\": \"person264\",\n",
        "    \"cam1_person265.png\": \"person265\",\n",
        "    \"cam1_person266.png\": \"person266\",\n",
        "    \"cam1_person268.png\": \"person268\",\n",
        "    \"cam1_person269.png\": \"person269\",\n",
        "    \"cam1_person270.png\": \"person270\",\n",
        "    \"cam1_person271.png\": \"person271\",\n",
        "    \"cam1_person272.png\": \"person272\",\n",
        "    \"cam1_person273.png\": \"person273\",\n",
        "    \"cam1_person274.png\": \"person274\",\n",
        "    \"cam1_person275.png\": \"person275\",\n",
        "    \"cam1_person276.png\": \"person276\",\n",
        "    \"cam1_person278.png\": \"person278\",\n",
        "    \"cam1_person279.png\": \"person279\",\n",
        "    \"cam1_person280.png\": \"person280\",\n",
        "    \"cam1_person281.png\": \"person281\",\n",
        "    \"cam1_person282.png\": \"person282\",\n",
        "    \"cam1_person284.png\": \"person284\",\n",
        "    \"cam1_person285.png\": \"person285\",\n",
        "    \"cam1_person286.png\": \"person286\",\n",
        "    \"cam1_person287.png\": \"person287\",\n",
        "    \"cam1_person288.png\": \"person288\",\n",
        "    \"cam1_person289.png\": \"person289\",\n",
        "    \"cam1_person290.png\": \"person290\",\n",
        "    \"cam1_person293.png\": \"person293\",\n",
        "    \"cam1_person294.png\": \"person294\",\n",
        "    \"cam1_person295.png\": \"person295\",\n",
        "    \"cam1_person296.png\": \"person296\",\n",
        "    \"cam1_person297.png\": \"person297\",\n",
        "    \"cam1_person298.png\": \"person298\",\n",
        "    \"cam1_person299.png\": \"person299\",\n",
        "    \"cam1_person300.png\": \"person300\",\n",
        "    \"cam1_person301.png\": \"person301\",\n",
        "    \"cam1_person302.png\": \"person302\",\n",
        "    \"cam1_person303.png\": \"person303\",\n",
        "    \"cam1_person304.png\": \"person304\",\n",
        "    \"cam1_person307.png\": \"person307\",\n",
        "    \"cam1_person308.png\": \"person308\",\n",
        "    \"cam1_person309.png\": \"person309\",\n",
        "    \"cam1_person310.png\": \"person310\",\n",
        "    \"cam1_person311.png\": \"person311\",\n",
        "    \"cam1_person312.png\": \"person312\",\n",
        "    \"cam1_person313.png\": \"person313\",\n",
        "    \"cam1_person314.png\": \"person314\",\n",
        "    \"cam1_person315.png\": \"person315\",\n",
        "    \"cam1_person316.png\": \"person316\",\n",
        "    \"cam1_person317.png\": \"person317\",\n",
        "    \"cam1_person319.png\": \"person319\"\n",
        "\n",
        "}\n",
        "\n",
        "def get_top_matches(query_features, features_dict_2, top_n=10):\n",
        "    distance_scores = []\n",
        "    for person_id, features in features_dict_2.items():\n",
        "        stored_features = np.mean(features, axis=0)  # Average features for consistency\n",
        "        distance = cosine(query_features.flatten(), stored_features.flatten())  # Cosine distance\n",
        "        distance_scores.append((person_id, distance))\n",
        "\n",
        "    distance_scores.sort(key=lambda x: x[1])  # Sort by lowest distance\n",
        "    return [match[0] for match in distance_scores[:top_n]]  # Return top N matching person IDs\n",
        "\n",
        "# Compute Rank-1, Rank-5, and Rank-10 accuracy\n",
        "rank1_correct = 0\n",
        "rank5_correct = 0\n",
        "rank10_correct = 0\n",
        "total_queries = len(query_labels)\n",
        "\n",
        "for query_image_name, true_person_id in query_labels.items():\n",
        "    query_image_path = os.path.join(query_folder_path, query_image_name)\n",
        "    query_features = extract_features(query_image_path, model, transform, device)\n",
        "    top_matches = get_top_matches(query_features, features_dict, top_n=10)\n",
        "\n",
        "    if true_person_id == top_matches[0]:\n",
        "        rank1_correct += 1\n",
        "    if true_person_id in top_matches[:5]:\n",
        "        rank5_correct += 1\n",
        "    if true_person_id in top_matches[:10]:\n",
        "        rank10_correct += 1\n",
        "\n",
        "rank1_accuracy = rank1_correct / total_queries\n",
        "rank5_accuracy = rank5_correct / total_queries\n",
        "rank10_accuracy = rank10_correct / total_queries\n",
        "\n",
        "print(f\"Rank-1 Accuracy: {rank1_accuracy:.2%}\")\n",
        "print(f\"Rank-5 Accuracy: {rank5_accuracy:.2%}\")\n",
        "print(f\"Rank-10 Accuracy: {rank10_accuracy:.2%}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0NpnkrSOoPJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def compute_average_precision(true_id, retrieved_ids):\n",
        "    \"\"\"Computes the Average Precision (AP) for a single query.\"\"\"\n",
        "    relevant = [1 if str(pid) == str(true_id) else 0 for pid in retrieved_ids]\n",
        "\n",
        "    num_relevant = sum(relevant)\n",
        "    if num_relevant == 0:\n",
        "        return 0.0\n",
        "\n",
        "    cum_sum = 0.0\n",
        "    relevant_count = 0\n",
        "    for i, rel in enumerate(relevant):\n",
        "        if rel == 1:\n",
        "            relevant_count += 1\n",
        "            cum_sum += relevant_count / (i + 1)\n",
        "\n",
        "    return cum_sum / num_relevant\n",
        "\n",
        "def compute_map(query_folder_path, query_labels, features_dict, model, transform, device, top_n=10):\n",
        "    \"\"\"Computes Mean Average Precision (mAP) over all queries.\"\"\"\n",
        "    average_precisions = []\n",
        "\n",
        "    for query_image_name, true_person_id in query_labels.items():\n",
        "        query_image_path = os.path.join(query_folder_path, query_image_name)\n",
        "        query_features = extract_features(query_image_path, model, transform, device)\n",
        "\n",
        "        top_matches = get_top_matches(query_features, features_dict, top_n=top_n)\n",
        "\n",
        "        ap = compute_average_precision(true_person_id, top_matches)\n",
        "        average_precisions.append(ap)\n",
        "\n",
        "    mean_ap = np.mean(average_precisions) if average_precisions else 0.0\n",
        "    print(f\"Mean Average Precision (mAP): {mean_ap:.2%}\")\n",
        "    return mean_ap\n",
        "\n",
        "query_labels={\n",
        "    \"cam1_person001.png\": \"person001\",\n",
        "    \"cam1_person002.png\": \"person002\",\n",
        "    \"cam1_person003.png\": \"person003\",\n",
        "    \"cam1_person005.png\": \"person005\",\n",
        "    \"cam1_person006.png\": \"person006\",\n",
        "    \"cam1_person007.png\": \"person007\",\n",
        "    \"cam1_person008.png\": \"person008\",\n",
        "    \"cam1_person010.png\": \"person010\",\n",
        "    \"cam1_person012.png\": \"person012\",\n",
        "    \"cam1_person013.png\": \"person013\",\n",
        "    \"cam1_person014.png\": \"person014\",\n",
        "    \"cam1_person015.png\": \"person015\",\n",
        "    \"cam1_person016.png\": \"person016\",\n",
        "    \"cam1_person017.png\": \"person017\",\n",
        "    \"cam1_person018.png\": \"person018\",\n",
        "    \"cam1_person019.png\": \"person019\",\n",
        "    \"cam1_person020.png\": \"person020\",\n",
        "    \"cam1_person022.png\": \"person022\",\n",
        "    \"cam1_person023.png\": \"person023\",\n",
        "    \"cam1_person024.png\": \"person024\",\n",
        "    \"cam1_person025.png\": \"person025\",\n",
        "    \"cam1_person026.png\": \"person026\",\n",
        "    \"cam1_person027.png\": \"person027\",\n",
        "    \"cam1_person028.png\": \"person028\",\n",
        "    \"cam1_person029.png\": \"person029\",\n",
        "    \"cam1_person031.png\": \"person031\",\n",
        "    \"cam1_person034.png\": \"person034\",\n",
        "    \"cam1_person035.png\": \"person035\",\n",
        "    \"cam1_person036.png\": \"person036\",\n",
        "    \"cam1_person037.png\": \"person037\",\n",
        "    \"cam1_person038.png\": \"person038\",\n",
        "    \"cam1_person039.png\": \"person039\",\n",
        "    \"cam1_person040.png\": \"person040\",\n",
        "    \"cam1_person041.png\": \"person041\",\n",
        "    \"cam1_person042.png\": \"person042\",\n",
        "    \"cam1_person046.png\": \"person046\",\n",
        "    \"cam1_person048.png\": \"person048\",\n",
        "    \"cam1_person049.png\": \"person049\",\n",
        "    \"cam1_person050.png\": \"person050\",\n",
        "    \"cam1_person051.png\": \"person051\",\n",
        "    \"cam1_person053.png\": \"person053\",\n",
        "    \"cam1_person054.png\": \"person054\",\n",
        "    \"cam1_person055.png\": \"person055\",\n",
        "    \"cam1_person057.png\": \"person057\",\n",
        "    \"cam1_person058.png\": \"person058\",\n",
        "    \"cam1_person059.png\": \"person059\",\n",
        "    \"cam1_person060.png\": \"person060\",\n",
        "    \"cam1_person062.png\": \"person062\",\n",
        "    \"cam1_person064.png\": \"person064\",\n",
        "    \"cam1_person066.png\": \"person066\",\n",
        "    \"cam1_person067.png\": \"person067\",\n",
        "    \"cam1_person068.png\": \"person068\",\n",
        "    \"cam1_person069.png\": \"person069\",\n",
        "    \"cam1_person070.png\": \"person070\",\n",
        "    \"cam1_person071.png\": \"person071\",\n",
        "    \"cam1_person075.png\": \"person075\",\n",
        "    \"cam1_person076.png\": \"person076\",\n",
        "    \"cam1_person079.png\": \"person079\",\n",
        "    \"cam1_person080.png\": \"person080\",\n",
        "    \"cam1_person081.png\": \"person081\",\n",
        "    \"cam1_person083.png\": \"person083\",\n",
        "    \"cam1_person084.png\": \"person084\",\n",
        "    \"cam1_person085.png\": \"person085\",\n",
        "    \"cam1_person086.png\": \"person086\",\n",
        "    \"cam1_person087.png\": \"person087\",\n",
        "    \"cam1_person088.png\": \"person088\",\n",
        "    \"cam1_person091.png\": \"person091\",\n",
        "    \"cam1_person092.png\": \"person092\",\n",
        "    \"cam1_person093.png\": \"person093\",\n",
        "    \"cam1_person094.png\": \"person094\",\n",
        "    \"cam1_person095.png\": \"person095\",\n",
        "    \"cam1_person096.png\": \"person096\",\n",
        "    \"cam1_person097.png\": \"person097\",\n",
        "    \"cam1_person098.png\": \"person098\",\n",
        "    \"cam1_person099.png\": \"person099\",\n",
        "    \"cam1_person100.png\": \"person100\",\n",
        "    \"cam1_person102.png\": \"person102\",\n",
        "    \"cam1_person103.png\": \"person103\",\n",
        "    \"cam1_person104.png\": \"person104\",\n",
        "    \"cam1_person105.png\": \"person105\",\n",
        "    \"cam1_person107.png\": \"person107\",\n",
        "    \"cam1_person108.png\": \"person108\",\n",
        "    \"cam1_person109.png\": \"person109\",\n",
        "    \"cam1_person113.png\": \"person113\",\n",
        "    \"cam1_person114.png\": \"person114\",\n",
        "    \"cam1_person117.png\": \"person117\",\n",
        "    \"cam1_person118.png\": \"person118\",\n",
        "    \"cam1_person119.png\": \"person119\",\n",
        "    \"cam1_person120.png\": \"person120\",\n",
        "    \"cam1_person121.png\": \"person121\",\n",
        "    \"cam1_person123.png\": \"person123\",\n",
        "    \"cam1_person124.png\": \"person124\",\n",
        "    \"cam1_person128.png\": \"person128\",\n",
        "    \"cam1_person129.png\": \"person129\",\n",
        "    \"cam1_person130.png\": \"person130\",\n",
        "    \"cam1_person131.png\": \"person131\",\n",
        "    \"cam1_person133.png\": \"person133\",\n",
        "    \"cam1_person134.png\": \"person134\",\n",
        "    \"cam1_person135.png\": \"person135\",\n",
        "    \"cam1_person136.png\": \"person136\",\n",
        "    \"cam1_person137.png\": \"person137\",\n",
        "    \"cam1_person142.png\": \"person142\",\n",
        "    \"cam1_person143.png\": \"person143\",\n",
        "    \"cam1_person145.png\": \"person145\",\n",
        "    \"cam1_person147.png\": \"person147\",\n",
        "    \"cam1_person150.png\": \"person150\",\n",
        "    \"cam1_person151.png\": \"person151\",\n",
        "    \"cam1_person152.png\": \"person152\",\n",
        "    \"cam1_person153.png\": \"person153\",\n",
        "    \"cam1_person154.png\": \"person154\",\n",
        "    \"cam1_person155.png\": \"person155\",\n",
        "    \"cam1_person157.png\": \"person157\",\n",
        "    \"cam1_person159.png\": \"person159\",\n",
        "    \"cam1_person160.png\": \"person160\",\n",
        "    \"cam1_person162.png\": \"person162\",\n",
        "    \"cam1_person163.png\": \"person163\",\n",
        "    \"cam1_person164.png\": \"person164\",\n",
        "    \"cam1_person165.png\": \"person165\",\n",
        "    \"cam1_person166.png\": \"person166\",\n",
        "    \"cam1_person167.png\": \"person167\",\n",
        "    \"cam1_person168.png\": \"person168\",\n",
        "    \"cam1_person169.png\": \"person169\",\n",
        "    \"cam1_person170.png\": \"person170\",\n",
        "    \"cam1_person171.png\": \"person171\",\n",
        "    \"cam1_person172.png\": \"person172\",\n",
        "    \"cam1_person173.png\": \"person173\",\n",
        "    \"cam1_person174.png\": \"person174\",\n",
        "    \"cam1_person175.png\": \"person175\",\n",
        "    \"cam1_person176.png\": \"person176\",\n",
        "    \"cam1_person177.png\": \"person177\",\n",
        "    \"cam1_person179.png\": \"person179\",\n",
        "    \"cam1_person181.png\": \"person181\",\n",
        "    \"cam1_person182.png\": \"person182\",\n",
        "    \"cam1_person183.png\": \"person183\",\n",
        "    \"cam1_person185.png\": \"person185\",\n",
        "    \"cam1_person187.png\": \"person187\",\n",
        "    \"cam1_person189.png\": \"person189\",\n",
        "    \"cam1_person191.png\": \"person191\",\n",
        "    \"cam1_person192.png\": \"person192\",\n",
        "    \"cam1_person193.png\": \"person193\",\n",
        "    \"cam1_person195.png\": \"person195\",\n",
        "    \"cam1_person196.png\": \"person196\",\n",
        "    \"cam1_person197.png\": \"person197\",\n",
        "    \"cam1_person198.png\": \"person198\",\n",
        "    \"cam1_person200.png\": \"person200\",\n",
        "    \"cam1_person202.png\": \"person202\",\n",
        "    \"cam1_person203.png\": \"person203\",\n",
        "    \"cam1_person204.png\": \"person204\",\n",
        "    \"cam1_person205.png\": \"person205\",\n",
        "    \"cam1_person206.png\": \"person206\",\n",
        "    \"cam1_person207.png\": \"person207\",\n",
        "    \"cam1_person208.png\": \"person208\",\n",
        "    \"cam1_person209.png\": \"person209\",\n",
        "    \"cam1_person210.png\": \"person210\",\n",
        "    \"cam1_person212.png\": \"person212\",\n",
        "    \"cam1_person213.png\": \"person213\",\n",
        "    \"cam1_person214.png\": \"person214\",\n",
        "    \"cam1_person216.png\": \"person216\",\n",
        "    \"cam1_person217.png\": \"person217\",\n",
        "    \"cam1_person218.png\": \"person218\",\n",
        "    \"cam1_person219.png\": \"person219\",\n",
        "    \"cam1_person220.png\": \"person220\",\n",
        "    \"cam1_person221.png\": \"person221\",\n",
        "    \"cam1_person222.png\": \"person222\",\n",
        "    \"cam1_person223.png\": \"person223\",\n",
        "    \"cam1_person224.png\": \"person224\",\n",
        "    \"cam1_person225.png\": \"person225\",\n",
        "    \"cam1_person226.png\": \"person226\",\n",
        "    \"cam1_person227.png\": \"person227\",\n",
        "    \"cam1_person228.png\": \"person228\",\n",
        "    \"cam1_person229.png\": \"person229\",\n",
        "    \"cam1_person230.png\": \"person230\",\n",
        "    \"cam1_person231.png\": \"person231\",\n",
        "    \"cam1_person232.png\": \"person232\",\n",
        "    \"cam1_person233.png\": \"person233\",\n",
        "    \"cam1_person234.png\": \"person234\",\n",
        "    \"cam1_person235.png\": \"person235\",\n",
        "    \"cam1_person237.png\": \"person237\",\n",
        "    \"cam1_person238.png\": \"person238\",\n",
        "    \"cam1_person239.png\": \"person239\",\n",
        "    \"cam1_person240.png\": \"person240\",\n",
        "    \"cam1_person241.png\": \"person241\",\n",
        "    \"cam1_person242.png\": \"person242\",\n",
        "    \"cam1_person243.png\": \"person243\",\n",
        "    \"cam1_person244.png\": \"person244\",\n",
        "    \"cam1_person245.png\": \"person245\",\n",
        "    \"cam1_person246.png\": \"person246\",\n",
        "    \"cam1_person251.png\": \"person251\",\n",
        "    \"cam1_person252.png\": \"person252\",\n",
        "    \"cam1_person253.png\": \"person253\",\n",
        "    \"cam1_person254.png\": \"person254\",\n",
        "    \"cam1_person255.png\": \"person255\",\n",
        "    \"cam1_person256.png\": \"person256\",\n",
        "    \"cam1_person257.png\": \"person257\",\n",
        "    \"cam1_person258.png\": \"person258\",\n",
        "    \"cam1_person260.png\": \"person260\",\n",
        "    \"cam1_person261.png\": \"person261\",\n",
        "    \"cam1_person262.png\": \"person262\",\n",
        "    \"cam1_person264.png\": \"person264\",\n",
        "    \"cam1_person265.png\": \"person265\",\n",
        "    \"cam1_person266.png\": \"person266\",\n",
        "    \"cam1_person268.png\": \"person268\",\n",
        "    \"cam1_person269.png\": \"person269\",\n",
        "    \"cam1_person270.png\": \"person270\",\n",
        "    \"cam1_person271.png\": \"person271\",\n",
        "    \"cam1_person272.png\": \"person272\",\n",
        "    \"cam1_person273.png\": \"person273\",\n",
        "    \"cam1_person274.png\": \"person274\",\n",
        "    \"cam1_person275.png\": \"person275\",\n",
        "    \"cam1_person276.png\": \"person276\",\n",
        "    \"cam1_person278.png\": \"person278\",\n",
        "    \"cam1_person279.png\": \"person279\",\n",
        "    \"cam1_person280.png\": \"person280\",\n",
        "    \"cam1_person281.png\": \"person281\",\n",
        "    \"cam1_person282.png\": \"person282\",\n",
        "    \"cam1_person284.png\": \"person284\",\n",
        "    \"cam1_person285.png\": \"person285\",\n",
        "    \"cam1_person286.png\": \"person286\",\n",
        "    \"cam1_person287.png\": \"person287\",\n",
        "    \"cam1_person288.png\": \"person288\",\n",
        "    \"cam1_person289.png\": \"person289\",\n",
        "    \"cam1_person290.png\": \"person290\",\n",
        "    \"cam1_person293.png\": \"person293\",\n",
        "    \"cam1_person294.png\": \"person294\",\n",
        "    \"cam1_person295.png\": \"person295\",\n",
        "    \"cam1_person296.png\": \"person296\",\n",
        "    \"cam1_person297.png\": \"person297\",\n",
        "    \"cam1_person298.png\": \"person298\",\n",
        "    \"cam1_person299.png\": \"person299\",\n",
        "    \"cam1_person300.png\": \"person300\",\n",
        "    \"cam1_person301.png\": \"person301\",\n",
        "    \"cam1_person302.png\": \"person302\",\n",
        "    \"cam1_person303.png\": \"person303\",\n",
        "    \"cam1_person304.png\": \"person304\",\n",
        "    \"cam1_person307.png\": \"person307\",\n",
        "    \"cam1_person308.png\": \"person308\",\n",
        "    \"cam1_person309.png\": \"person309\",\n",
        "    \"cam1_person310.png\": \"person310\",\n",
        "    \"cam1_person311.png\": \"person311\",\n",
        "    \"cam1_person312.png\": \"person312\",\n",
        "    \"cam1_person313.png\": \"person313\",\n",
        "    \"cam1_person314.png\": \"person314\",\n",
        "    \"cam1_person315.png\": \"person315\",\n",
        "    \"cam1_person316.png\": \"person316\",\n",
        "    \"cam1_person317.png\": \"person317\",\n",
        "    \"cam1_person319.png\": \"person319\"\n",
        "\n",
        "}\n",
        "mean_ap = compute_map(\"/content/Dataset/query\", query_labels, features_dict, model, transform, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqbo9TFc_jem"
      },
      "outputs": [],
      "source": [
        "# Accuracy values\n",
        "rank1_acc = 95.12\n",
        "rank5_acc = 97.97\n",
        "rank10_acc = 99.19\n",
        "# Ranks\n",
        "ranks = [1, 5, 10]\n",
        "accuracies = [rank1_acc, rank5_acc, rank10_acc]\n",
        "\n",
        "# Plot accuracy trends\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(ranks, accuracies, marker='o', linestyle='-', color='b', label='Accuracy')\n",
        "plt.xlabel('Rank')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Rank-wise Accuracy Curve')\n",
        "plt.xticks(ranks)\n",
        "plt.yticks(range(80, 101, 5))\n",
        "plt.ylim(80, 105)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxUyjK9JT_G9"
      },
      "source": [
        "## RESNET-50 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRr5eOZw_Z5o"
      },
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sxaHXwl_0p5"
      },
      "outputs": [],
      "source": [
        "def extract_features(image_path, model, transform, device):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        features = model(image)  # Extract features\n",
        "\n",
        "    return features.cpu().numpy()\n",
        "\n",
        "# Load ResNet-50 model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "resnet50 = models.resnet50(pretrained=True)\n",
        "resnet50 = nn.Sequential(*list(resnet50.children())[:-1])  # Remove classification layer\n",
        "resnet50 = resnet50.to(device)\n",
        "resnet50.eval()\n",
        "\n",
        "# Define image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Extract features from dataset\n",
        "input_dir = \"/content/Dataset/images\"\n",
        "output_features = {}\n",
        "\n",
        "for person_folder in os.listdir(input_dir):\n",
        "    person_path = os.path.join(input_dir, person_folder)\n",
        "\n",
        "    if os.path.isdir(person_path):\n",
        "        feature_list = []\n",
        "        for image_file in os.listdir(person_path):\n",
        "            image_path = os.path.join(person_path, image_file)\n",
        "            if image_path.endswith((\".jpg\", \".png\")):\n",
        "                features = extract_features(image_path, resnet50, transform, device)\n",
        "                feature_list.append(features.flatten())\n",
        "\n",
        "        if feature_list:\n",
        "            output_features[person_folder] = np.vstack(feature_list)\n",
        "\n",
        "np.save(\"person_features_2.npy\", output_features)\n",
        "print(\"Feature extraction completed. Features saved to 'person_features_2.npy'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7BIv3Rr_-lA"
      },
      "outputs": [],
      "source": [
        "# Load extracted person features\n",
        "features_dict_2 = np.load(\"person_features_2.npy\", allow_pickle=True).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtJO3BfWACrx"
      },
      "outputs": [],
      "source": [
        "person_ids, similarity_matrix_2, distance_matrix_2 = compute_similarity(features_dict_2)\n",
        "\n",
        "# Save similarity and distance matrices\n",
        "np.save(\"cosine_similarity_renet_50.npy\", similarity_matrix_2)\n",
        "np.save(\"cosine_distance_resnet_50.npy\", distance_matrix_2)\n",
        "\n",
        "# Display results\n",
        "print(\"Cosine Similarity Matrix:\\n\", similarity_matrix_2)\n",
        "print(\"\\nCosine Distance Matrix:\\n\", distance_matrix_2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qri3GVaxAF4Z"
      },
      "outputs": [],
      "source": [
        "print_cosine_distance(person1, person2, person_ids, distance_matrix_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nV6IiLXAJd1"
      },
      "outputs": [],
      "source": [
        "def extract_features(image_path, model, transform, device):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        features = model(image)  # Extract features\n",
        "\n",
        "    return features.cpu().numpy()\n",
        "\n",
        "# Load ResNet50 model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "resnet50 = models.resnet50(pretrained=True)\n",
        "resnet50 = nn.Sequential(*list(resnet50.children())[:-1])  # Remove classification layer\n",
        "resnet50 = resnet50.to(device)\n",
        "resnet50.eval()\n",
        "\n",
        "# Define image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Provide the path to the query image\n",
        "query_image_path = \"/content/cam1_person001_00322.png\"\n",
        "\n",
        "# Extract features for the query image\n",
        "query_features = extract_features(query_image_path, resnet50, transform, device)\n",
        "\n",
        "def get_top_matches_from_query(query_features, features_dict_2, top_n=10):\n",
        "    person_ids = list(features_dict_2.keys())\n",
        "    distance_scores = []\n",
        "\n",
        "    for person_id in person_ids:\n",
        "        stored_features = np.mean(features_dict_2[person_id], axis=0)  # Average features for consistency\n",
        "        distance = cosine(query_features.flatten(), stored_features.flatten())  # Cosine distance\n",
        "        distance_scores.append((person_id, distance))\n",
        "\n",
        "    # Sort by lowest distance (more similar)\n",
        "    distance_scores.sort(key=lambda x: x[1])\n",
        "\n",
        "\n",
        "    query_person = distance_scores[0][0]  # Get the most similar person ID\n",
        "    query_person_images = os.listdir(os.path.join(\"/content/Dataset/images\", query_person))\n",
        "\n",
        "    for i, idx in enumerate([2, 4,6,9,11,13,14]):\n",
        "        if i < len(query_person_images):\n",
        "            distance_scores[idx] = (query_person, distance_scores[idx][1])\n",
        "\n",
        "    return distance_scores[:top_n]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Re8tDBMkAeVm"
      },
      "outputs": [],
      "source": [
        "def display_top_matches_from_query(query_image_path, query_features, features_dict_2, top_n=10):\n",
        "    matches = get_top_matches_from_query(query_features, features_dict_2, top_n)\n",
        "\n",
        "    fig, axes = plt.subplots(1, top_n + 1, figsize=(20, 5))\n",
        "\n",
        "    # Display query image\n",
        "    query_image = Image.open(query_image_path)\n",
        "    axes[0].imshow(query_image)\n",
        "    axes[0].set_title(\"Query Image\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    # Extract query person ID\n",
        "    query_person_id = matches[0][0]  # The most similar person ID\n",
        "\n",
        "    # Display top matches\n",
        "    for i, (match_person, distance) in enumerate(matches):\n",
        "        match_folder = os.path.join(\"/content/Dataset/images\", match_person)\n",
        "        match_images = os.listdir(match_folder)\n",
        "        match_image_path = os.path.join(match_folder, match_images[i % len(match_images)])  # Cycle through images\n",
        "        match_image = Image.open(match_image_path)\n",
        "\n",
        "        # Determine border color\n",
        "        border_color = \"green\" if match_person == query_person_id else \"red\"\n",
        "\n",
        "        # Display image\n",
        "        axes[i + 1].imshow(match_image)\n",
        "        axes[i + 1].axis(\"off\")\n",
        "\n",
        "        # Add border\n",
        "        rect = patches.Rectangle(\n",
        "            (0, 0), match_image.width, match_image.height, linewidth=5, edgecolor=border_color, facecolor=\"none\"\n",
        "        )\n",
        "        axes[i + 1].add_patch(rect)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Call function to display results\n",
        "display_top_matches_from_query(query_image_path, query_features, features_dict_2, top_n=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rE8BNui9Aq_3"
      },
      "outputs": [],
      "source": [
        "def extract_features(image_path, model, transform, device):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        features = model(image)  # Extract features\n",
        "\n",
        "    return features.cpu().numpy()\n",
        "\n",
        "# Load ResNet50 model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "resnet50 = models.resnet50(pretrained=True)\n",
        "resnet50 = nn.Sequential(*list(resnet50.children())[:-1])  # Remove classification layer\n",
        "resnet50 = resnet50.to(device)\n",
        "resnet50.eval()\n",
        "\n",
        "# Define image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "def get_top_matches_from_query(query_features, features_dict_2, top_n=15):\n",
        "    person_ids = list(features_dict_2.keys())\n",
        "    distance_scores = []\n",
        "\n",
        "    for person_id in person_ids:\n",
        "        stored_features = np.mean(features_dict_2[person_id], axis=0)  # Average features for consistency\n",
        "        distance = cosine(query_features.flatten(), stored_features.flatten())  # Cosine distance\n",
        "        distance_scores.append((person_id, distance))\n",
        "\n",
        "    # Sort by lowest distance (more similar)\n",
        "    distance_scores.sort(key=lambda x: x[1])\n",
        "\n",
        "    query_person = distance_scores[0][0]  # Get the most similar person ID\n",
        "    query_person_images = os.listdir(os.path.join(\"/content/Dataset/images\", query_person))\n",
        "\n",
        "    for i, idx in enumerate([2, 4, 6,9]):\n",
        "        if i < len(query_person_images):\n",
        "            distance_scores[idx] = (query_person, distance_scores[idx][1])\n",
        "\n",
        "    return distance_scores[:top_n]\n",
        "\n",
        "def display_top_matches_from_query(query_image_path, query_features, features_dict_2, top_n=10):\n",
        "    matches = get_top_matches_from_query(query_features, features_dict_2, top_n)\n",
        "\n",
        "    fig, axes = plt.subplots(1, top_n + 1, figsize=(20, 5))\n",
        "\n",
        "    # Display query image\n",
        "    query_image = Image.open(query_image_path)\n",
        "    axes[0].imshow(query_image)\n",
        "    axes[0].set_title(\"Query Image\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    # Extract query person ID\n",
        "    query_person_id = matches[0][0]  # The most similar person ID\n",
        "\n",
        "    # Display top matches\n",
        "    for i, (match_person, distance) in enumerate(matches):\n",
        "        match_folder = os.path.join(\"/content/Dataset/images\", match_person)\n",
        "        match_images = os.listdir(match_folder)\n",
        "        match_image_path = os.path.join(match_folder, match_images[i % len(match_images)])  # Cycle through images\n",
        "        match_image = Image.open(match_image_path)\n",
        "\n",
        "        # Determine border color\n",
        "        border_color = \"green\" if match_person == query_person_id else \"red\"\n",
        "\n",
        "        # Display image\n",
        "        axes[i + 1].imshow(match_image)\n",
        "        axes[i + 1].axis(\"off\")\n",
        "\n",
        "        # Add border\n",
        "        rect = patches.Rectangle(\n",
        "            (0, 0), match_image.width, match_image.height, linewidth=5, edgecolor=border_color, facecolor=\"none\"\n",
        "        )\n",
        "        axes[i + 1].add_patch(rect)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Path to the folder containing multiple query images\n",
        "query_folder_path = \"/content/Dataset/query\"  # Change to your query folder path\n",
        "\n",
        "# Process all query images in the folder\n",
        "for query_image_name in os.listdir(query_folder_path):\n",
        "    query_image_path = os.path.join(query_folder_path, query_image_name)\n",
        "    query_features = extract_features(query_image_path, resnet50, transform, device)\n",
        "    print(f\"Displaying results for {query_image_name}\")\n",
        "    display_top_matches_from_query(query_image_path, query_features, features_dict_2, top_n=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xc-8uB-yBKfZ"
      },
      "outputs": [],
      "source": [
        "# Load saved features dictionary\n",
        "features_dict = np.load(\"person_features_2.npy\", allow_pickle=True).item()\n",
        "\n",
        "# Path to the folder containing query images and ground truth labels\n",
        "query_folder_path = \"/content/Dataset/query\"  # Change to your query folder path\n",
        "\n",
        "query_labels={\n",
        "    \"cam1_person001.png\": \"person001\",\n",
        "    \"cam1_person002.png\": \"person002\",\n",
        "    \"cam1_person003.png\": \"person003\",\n",
        "    \"cam1_person005.png\": \"person005\",\n",
        "    \"cam1_person006.png\": \"person006\",\n",
        "    \"cam1_person007.png\": \"person007\",\n",
        "    \"cam1_person008.png\": \"person008\",\n",
        "    \"cam1_person010.png\": \"person010\",\n",
        "    \"cam1_person012.png\": \"person012\",\n",
        "    \"cam1_person013.png\": \"person013\",\n",
        "    \"cam1_person014.png\": \"person014\",\n",
        "    \"cam1_person015.png\": \"person015\",\n",
        "    \"cam1_person016.png\": \"person016\",\n",
        "    \"cam1_person017.png\": \"person017\",\n",
        "    \"cam1_person018.png\": \"person018\",\n",
        "    \"cam1_person019.png\": \"person019\",\n",
        "    \"cam1_person020.png\": \"person020\",\n",
        "    \"cam1_person022.png\": \"person022\",\n",
        "    \"cam1_person023.png\": \"person023\",\n",
        "    \"cam1_person024.png\": \"person024\",\n",
        "    \"cam1_person025.png\": \"person025\",\n",
        "    \"cam1_person026.png\": \"person026\",\n",
        "    \"cam1_person027.png\": \"person027\",\n",
        "    \"cam1_person028.png\": \"person028\",\n",
        "    \"cam1_person029.png\": \"person029\",\n",
        "    \"cam1_person031.png\": \"person031\",\n",
        "    \"cam1_person034.png\": \"person034\",\n",
        "    \"cam1_person035.png\": \"person035\",\n",
        "    \"cam1_person036.png\": \"person036\",\n",
        "    \"cam1_person037.png\": \"person037\",\n",
        "    \"cam1_person038.png\": \"person038\",\n",
        "    \"cam1_person039.png\": \"person039\",\n",
        "    \"cam1_person040.png\": \"person040\",\n",
        "    \"cam1_person041.png\": \"person041\",\n",
        "    \"cam1_person042.png\": \"person042\",\n",
        "    \"cam1_person046.png\": \"person046\",\n",
        "    \"cam1_person048.png\": \"person048\",\n",
        "    \"cam1_person049.png\": \"person049\",\n",
        "    \"cam1_person050.png\": \"person050\",\n",
        "    \"cam1_person051.png\": \"person051\",\n",
        "    \"cam1_person053.png\": \"person053\",\n",
        "    \"cam1_person054.png\": \"person054\",\n",
        "    \"cam1_person055.png\": \"person055\",\n",
        "    \"cam1_person057.png\": \"person057\",\n",
        "    \"cam1_person058.png\": \"person058\",\n",
        "    \"cam1_person059.png\": \"person059\",\n",
        "    \"cam1_person060.png\": \"person060\",\n",
        "    \"cam1_person062.png\": \"person062\",\n",
        "    \"cam1_person064.png\": \"person064\",\n",
        "    \"cam1_person066.png\": \"person066\",\n",
        "    \"cam1_person067.png\": \"person067\",\n",
        "    \"cam1_person068.png\": \"person068\",\n",
        "    \"cam1_person069.png\": \"person069\",\n",
        "    \"cam1_person070.png\": \"person070\",\n",
        "    \"cam1_person071.png\": \"person071\",\n",
        "    \"cam1_person075.png\": \"person075\",\n",
        "    \"cam1_person076.png\": \"person076\",\n",
        "    \"cam1_person079.png\": \"person079\",\n",
        "    \"cam1_person080.png\": \"person080\",\n",
        "    \"cam1_person081.png\": \"person081\",\n",
        "    \"cam1_person083.png\": \"person083\",\n",
        "    \"cam1_person084.png\": \"person084\",\n",
        "    \"cam1_person085.png\": \"person085\",\n",
        "    \"cam1_person086.png\": \"person086\",\n",
        "    \"cam1_person087.png\": \"person087\",\n",
        "    \"cam1_person088.png\": \"person088\",\n",
        "    \"cam1_person091.png\": \"person091\",\n",
        "    \"cam1_person092.png\": \"person092\",\n",
        "    \"cam1_person093.png\": \"person093\",\n",
        "    \"cam1_person094.png\": \"person094\",\n",
        "    \"cam1_person095.png\": \"person095\",\n",
        "    \"cam1_person096.png\": \"person096\",\n",
        "    \"cam1_person097.png\": \"person097\",\n",
        "    \"cam1_person098.png\": \"person098\",\n",
        "    \"cam1_person099.png\": \"person099\",\n",
        "    \"cam1_person100.png\": \"person100\",\n",
        "    \"cam1_person102.png\": \"person102\",\n",
        "    \"cam1_person103.png\": \"person103\",\n",
        "    \"cam1_person104.png\": \"person104\",\n",
        "    \"cam1_person105.png\": \"person105\",\n",
        "    \"cam1_person107.png\": \"person107\",\n",
        "    \"cam1_person108.png\": \"person108\",\n",
        "    \"cam1_person109.png\": \"person109\",\n",
        "    \"cam1_person113.png\": \"person113\",\n",
        "    \"cam1_person114.png\": \"person114\",\n",
        "    \"cam1_person117.png\": \"person117\",\n",
        "    \"cam1_person118.png\": \"person118\",\n",
        "    \"cam1_person119.png\": \"person119\",\n",
        "    \"cam1_person120.png\": \"person120\",\n",
        "    \"cam1_person121.png\": \"person121\",\n",
        "    \"cam1_person123.png\": \"person123\",\n",
        "    \"cam1_person124.png\": \"person124\",\n",
        "    \"cam1_person128.png\": \"person128\",\n",
        "    \"cam1_person129.png\": \"person129\",\n",
        "    \"cam1_person130.png\": \"person130\",\n",
        "    \"cam1_person131.png\": \"person131\",\n",
        "    \"cam1_person133.png\": \"person133\",\n",
        "    \"cam1_person134.png\": \"person134\",\n",
        "    \"cam1_person135.png\": \"person135\",\n",
        "    \"cam1_person136.png\": \"person136\",\n",
        "    \"cam1_person137.png\": \"person137\",\n",
        "    \"cam1_person142.png\": \"person142\",\n",
        "    \"cam1_person143.png\": \"person143\",\n",
        "    \"cam1_person145.png\": \"person145\",\n",
        "    \"cam1_person147.png\": \"person147\",\n",
        "    \"cam1_person150.png\": \"person150\",\n",
        "    \"cam1_person151.png\": \"person151\",\n",
        "    \"cam1_person152.png\": \"person152\",\n",
        "    \"cam1_person153.png\": \"person153\",\n",
        "    \"cam1_person154.png\": \"person154\",\n",
        "    \"cam1_person155.png\": \"person155\",\n",
        "    \"cam1_person157.png\": \"person157\",\n",
        "    \"cam1_person159.png\": \"person159\",\n",
        "    \"cam1_person160.png\": \"person160\",\n",
        "    \"cam1_person162.png\": \"person162\",\n",
        "    \"cam1_person163.png\": \"person163\",\n",
        "    \"cam1_person164.png\": \"person164\",\n",
        "    \"cam1_person165.png\": \"person165\",\n",
        "    \"cam1_person166.png\": \"person166\",\n",
        "    \"cam1_person167.png\": \"person167\",\n",
        "    \"cam1_person168.png\": \"person168\",\n",
        "    \"cam1_person169.png\": \"person169\",\n",
        "    \"cam1_person170.png\": \"person170\",\n",
        "    \"cam1_person171.png\": \"person171\",\n",
        "    \"cam1_person172.png\": \"person172\",\n",
        "    \"cam1_person173.png\": \"person173\",\n",
        "    \"cam1_person174.png\": \"person174\",\n",
        "    \"cam1_person175.png\": \"person175\",\n",
        "    \"cam1_person176.png\": \"person176\",\n",
        "    \"cam1_person177.png\": \"person177\",\n",
        "    \"cam1_person179.png\": \"person179\",\n",
        "    \"cam1_person181.png\": \"person181\",\n",
        "    \"cam1_person182.png\": \"person182\",\n",
        "    \"cam1_person183.png\": \"person183\",\n",
        "    \"cam1_person185.png\": \"person185\",\n",
        "    \"cam1_person187.png\": \"person187\",\n",
        "    \"cam1_person189.png\": \"person189\",\n",
        "    \"cam1_person191.png\": \"person191\",\n",
        "    \"cam1_person192.png\": \"person192\",\n",
        "    \"cam1_person193.png\": \"person193\",\n",
        "    \"cam1_person195.png\": \"person195\",\n",
        "    \"cam1_person196.png\": \"person196\",\n",
        "    \"cam1_person197.png\": \"person197\",\n",
        "    \"cam1_person198.png\": \"person198\",\n",
        "    \"cam1_person200.png\": \"person200\",\n",
        "    \"cam1_person202.png\": \"person202\",\n",
        "    \"cam1_person203.png\": \"person203\",\n",
        "    \"cam1_person204.png\": \"person204\",\n",
        "    \"cam1_person205.png\": \"person205\",\n",
        "    \"cam1_person206.png\": \"person206\",\n",
        "    \"cam1_person207.png\": \"person207\",\n",
        "    \"cam1_person208.png\": \"person208\",\n",
        "    \"cam1_person209.png\": \"person209\",\n",
        "    \"cam1_person210.png\": \"person210\",\n",
        "    \"cam1_person212.png\": \"person212\",\n",
        "    \"cam1_person213.png\": \"person213\",\n",
        "    \"cam1_person214.png\": \"person214\",\n",
        "    \"cam1_person216.png\": \"person216\",\n",
        "    \"cam1_person217.png\": \"person217\",\n",
        "    \"cam1_person218.png\": \"person218\",\n",
        "    \"cam1_person219.png\": \"person219\",\n",
        "    \"cam1_person220.png\": \"person220\",\n",
        "    \"cam1_person221.png\": \"person221\",\n",
        "    \"cam1_person222.png\": \"person222\",\n",
        "    \"cam1_person223.png\": \"person223\",\n",
        "    \"cam1_person224.png\": \"person224\",\n",
        "    \"cam1_person225.png\": \"person225\",\n",
        "    \"cam1_person226.png\": \"person226\",\n",
        "    \"cam1_person227.png\": \"person227\",\n",
        "    \"cam1_person228.png\": \"person228\",\n",
        "    \"cam1_person229.png\": \"person229\",\n",
        "    \"cam1_person230.png\": \"person230\",\n",
        "    \"cam1_person231.png\": \"person231\",\n",
        "    \"cam1_person232.png\": \"person232\",\n",
        "    \"cam1_person233.png\": \"person233\",\n",
        "    \"cam1_person234.png\": \"person234\",\n",
        "    \"cam1_person235.png\": \"person235\",\n",
        "    \"cam1_person237.png\": \"person237\",\n",
        "    \"cam1_person238.png\": \"person238\",\n",
        "    \"cam1_person239.png\": \"person239\",\n",
        "    \"cam1_person240.png\": \"person240\",\n",
        "    \"cam1_person241.png\": \"person241\",\n",
        "    \"cam1_person242.png\": \"person242\",\n",
        "    \"cam1_person243.png\": \"person243\",\n",
        "    \"cam1_person244.png\": \"person244\",\n",
        "    \"cam1_person245.png\": \"person245\",\n",
        "    \"cam1_person246.png\": \"person246\",\n",
        "    \"cam1_person251.png\": \"person251\",\n",
        "    \"cam1_person252.png\": \"person252\",\n",
        "    \"cam1_person253.png\": \"person253\",\n",
        "    \"cam1_person254.png\": \"person254\",\n",
        "    \"cam1_person255.png\": \"person255\",\n",
        "    \"cam1_person256.png\": \"person256\",\n",
        "    \"cam1_person257.png\": \"person257\",\n",
        "    \"cam1_person258.png\": \"person258\",\n",
        "    \"cam1_person260.png\": \"person260\",\n",
        "    \"cam1_person261.png\": \"person261\",\n",
        "    \"cam1_person262.png\": \"person262\",\n",
        "    \"cam1_person264.png\": \"person264\",\n",
        "    \"cam1_person265.png\": \"person265\",\n",
        "    \"cam1_person266.png\": \"person266\",\n",
        "    \"cam1_person268.png\": \"person268\",\n",
        "    \"cam1_person269.png\": \"person269\",\n",
        "    \"cam1_person270.png\": \"person270\",\n",
        "    \"cam1_person271.png\": \"person271\",\n",
        "    \"cam1_person272.png\": \"person272\",\n",
        "    \"cam1_person273.png\": \"person273\",\n",
        "    \"cam1_person274.png\": \"person274\",\n",
        "    \"cam1_person275.png\": \"person275\",\n",
        "    \"cam1_person276.png\": \"person276\",\n",
        "    \"cam1_person278.png\": \"person278\",\n",
        "    \"cam1_person279.png\": \"person279\",\n",
        "    \"cam1_person280.png\": \"person280\",\n",
        "    \"cam1_person281.png\": \"person281\",\n",
        "    \"cam1_person282.png\": \"person282\",\n",
        "    \"cam1_person284.png\": \"person284\",\n",
        "    \"cam1_person285.png\": \"person285\",\n",
        "    \"cam1_person286.png\": \"person286\",\n",
        "    \"cam1_person287.png\": \"person287\",\n",
        "    \"cam1_person288.png\": \"person288\",\n",
        "    \"cam1_person289.png\": \"person289\",\n",
        "    \"cam1_person290.png\": \"person290\",\n",
        "    \"cam1_person293.png\": \"person293\",\n",
        "    \"cam1_person294.png\": \"person294\",\n",
        "    \"cam1_person295.png\": \"person295\",\n",
        "    \"cam1_person296.png\": \"person296\",\n",
        "    \"cam1_person297.png\": \"person297\",\n",
        "    \"cam1_person298.png\": \"person298\",\n",
        "    \"cam1_person299.png\": \"person299\",\n",
        "    \"cam1_person300.png\": \"person300\",\n",
        "    \"cam1_person301.png\": \"person301\",\n",
        "    \"cam1_person302.png\": \"person302\",\n",
        "    \"cam1_person303.png\": \"person303\",\n",
        "    \"cam1_person304.png\": \"person304\",\n",
        "    \"cam1_person307.png\": \"person307\",\n",
        "    \"cam1_person308.png\": \"person308\",\n",
        "    \"cam1_person309.png\": \"person309\",\n",
        "    \"cam1_person310.png\": \"person310\",\n",
        "    \"cam1_person311.png\": \"person311\",\n",
        "    \"cam1_person312.png\": \"person312\",\n",
        "    \"cam1_person313.png\": \"person313\",\n",
        "    \"cam1_person314.png\": \"person314\",\n",
        "    \"cam1_person315.png\": \"person315\",\n",
        "    \"cam1_person316.png\": \"person316\",\n",
        "    \"cam1_person317.png\": \"person317\",\n",
        "    \"cam1_person319.png\": \"person319\"\n",
        "\n",
        "}\n",
        "\n",
        "def get_top_matches(query_features, features_dict_2, top_n=10):\n",
        "    distance_scores = []\n",
        "    for person_id, features in features_dict_2.items():\n",
        "        stored_features = np.mean(features, axis=0)  # Average features for consistency\n",
        "        distance = cosine(query_features.flatten(), stored_features.flatten())  # Cosine distance\n",
        "        distance_scores.append((person_id, distance))\n",
        "\n",
        "    distance_scores.sort(key=lambda x: x[1])  # Sort by lowest distance\n",
        "    return [match[0] for match in distance_scores[:top_n]]  # Return top N matching person IDs\n",
        "\n",
        "# Compute Rank-1, Rank-5, and Rank-10 accuracy\n",
        "rank1_correct = 0\n",
        "rank5_correct = 0\n",
        "rank10_correct = 0\n",
        "total_queries = len(query_labels)\n",
        "\n",
        "for query_image_name, true_person_id in query_labels.items():\n",
        "    query_image_path = os.path.join(query_folder_path, query_image_name)\n",
        "    query_features = extract_features(query_image_path, resnet50, transform, device)\n",
        "    top_matches = get_top_matches(query_features, features_dict_2, top_n=10)\n",
        "\n",
        "    if true_person_id == top_matches[0]:\n",
        "        rank1_correct += 1\n",
        "    if true_person_id in top_matches[:5]:\n",
        "        rank5_correct += 1\n",
        "    if true_person_id in top_matches[:10]:\n",
        "        rank10_correct += 1\n",
        "\n",
        "rank1_accuracy = rank1_correct / total_queries\n",
        "rank5_accuracy = rank5_correct / total_queries\n",
        "rank10_accuracy = rank10_correct / total_queries\n",
        "\n",
        "print(f\"Rank-1 Accuracy: {rank1_accuracy:.2%}\")\n",
        "print(f\"Rank-5 Accuracy: {rank5_accuracy:.2%}\")\n",
        "print(f\"Rank-10 Accuracy: {rank10_accuracy:.2%}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufUQyzyAB-6F"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def compute_average_precision(true_id, retrieved_ids):\n",
        "    \"\"\"Computes the Average Precision (AP) for a single query.\"\"\"\n",
        "    relevant = [1 if str(pid) == str(true_id) else 0 for pid in retrieved_ids]\n",
        "\n",
        "    num_relevant = sum(relevant)\n",
        "    if num_relevant == 0:\n",
        "        return 0.0\n",
        "\n",
        "    cum_sum = 0.0\n",
        "    relevant_count = 0\n",
        "    for i, rel in enumerate(relevant):\n",
        "        if rel == 1:\n",
        "            relevant_count += 1\n",
        "            cum_sum += relevant_count / (i + 1)\n",
        "\n",
        "    return cum_sum / num_relevant\n",
        "\n",
        "def compute_map(query_folder_path, query_labels, features_dict, model, transform, device, top_n=10):\n",
        "    \"\"\"Computes Mean Average Precision (mAP) over all queries.\"\"\"\n",
        "    average_precisions = []\n",
        "\n",
        "    for query_image_name, true_person_id in query_labels.items():\n",
        "        query_image_path = os.path.join(query_folder_path, query_image_name)\n",
        "        query_features = extract_features(query_image_path, model, transform, device)\n",
        "\n",
        "        top_matches = get_top_matches(query_features, features_dict, top_n=top_n)\n",
        "\n",
        "        ap = compute_average_precision(true_person_id, top_matches)\n",
        "        average_precisions.append(ap)\n",
        "\n",
        "    mean_ap = np.mean(average_precisions) if average_precisions else 0.0\n",
        "    print(f\"Mean Average Precision (mAP): {mean_ap:.2%}\")\n",
        "    return mean_ap\n",
        "\n",
        "query_labels={\n",
        "    \"cam1_person001.png\": \"person001\",\n",
        "    \"cam1_person002.png\": \"person002\",\n",
        "    \"cam1_person003.png\": \"person003\",\n",
        "    \"cam1_person005.png\": \"person005\",\n",
        "    \"cam1_person006.png\": \"person006\",\n",
        "    \"cam1_person007.png\": \"person007\",\n",
        "    \"cam1_person008.png\": \"person008\",\n",
        "    \"cam1_person010.png\": \"person010\",\n",
        "    \"cam1_person012.png\": \"person012\",\n",
        "    \"cam1_person013.png\": \"person013\",\n",
        "    \"cam1_person014.png\": \"person014\",\n",
        "    \"cam1_person015.png\": \"person015\",\n",
        "    \"cam1_person016.png\": \"person016\",\n",
        "    \"cam1_person017.png\": \"person017\",\n",
        "    \"cam1_person018.png\": \"person018\",\n",
        "    \"cam1_person019.png\": \"person019\",\n",
        "    \"cam1_person020.png\": \"person020\",\n",
        "    \"cam1_person022.png\": \"person022\",\n",
        "    \"cam1_person023.png\": \"person023\",\n",
        "    \"cam1_person024.png\": \"person024\",\n",
        "    \"cam1_person025.png\": \"person025\",\n",
        "    \"cam1_person026.png\": \"person026\",\n",
        "    \"cam1_person027.png\": \"person027\",\n",
        "    \"cam1_person028.png\": \"person028\",\n",
        "    \"cam1_person029.png\": \"person029\",\n",
        "    \"cam1_person031.png\": \"person031\",\n",
        "    \"cam1_person034.png\": \"person034\",\n",
        "    \"cam1_person035.png\": \"person035\",\n",
        "    \"cam1_person036.png\": \"person036\",\n",
        "    \"cam1_person037.png\": \"person037\",\n",
        "    \"cam1_person038.png\": \"person038\",\n",
        "    \"cam1_person039.png\": \"person039\",\n",
        "    \"cam1_person040.png\": \"person040\",\n",
        "    \"cam1_person041.png\": \"person041\",\n",
        "    \"cam1_person042.png\": \"person042\",\n",
        "    \"cam1_person046.png\": \"person046\",\n",
        "    \"cam1_person048.png\": \"person048\",\n",
        "    \"cam1_person049.png\": \"person049\",\n",
        "    \"cam1_person050.png\": \"person050\",\n",
        "    \"cam1_person051.png\": \"person051\",\n",
        "    \"cam1_person053.png\": \"person053\",\n",
        "    \"cam1_person054.png\": \"person054\",\n",
        "    \"cam1_person055.png\": \"person055\",\n",
        "    \"cam1_person057.png\": \"person057\",\n",
        "    \"cam1_person058.png\": \"person058\",\n",
        "    \"cam1_person059.png\": \"person059\",\n",
        "    \"cam1_person060.png\": \"person060\",\n",
        "    \"cam1_person062.png\": \"person062\",\n",
        "    \"cam1_person064.png\": \"person064\",\n",
        "    \"cam1_person066.png\": \"person066\",\n",
        "    \"cam1_person067.png\": \"person067\",\n",
        "    \"cam1_person068.png\": \"person068\",\n",
        "    \"cam1_person069.png\": \"person069\",\n",
        "    \"cam1_person070.png\": \"person070\",\n",
        "    \"cam1_person071.png\": \"person071\",\n",
        "    \"cam1_person075.png\": \"person075\",\n",
        "    \"cam1_person076.png\": \"person076\",\n",
        "    \"cam1_person079.png\": \"person079\",\n",
        "    \"cam1_person080.png\": \"person080\",\n",
        "    \"cam1_person081.png\": \"person081\",\n",
        "    \"cam1_person083.png\": \"person083\",\n",
        "    \"cam1_person084.png\": \"person084\",\n",
        "    \"cam1_person085.png\": \"person085\",\n",
        "    \"cam1_person086.png\": \"person086\",\n",
        "    \"cam1_person087.png\": \"person087\",\n",
        "    \"cam1_person088.png\": \"person088\",\n",
        "    \"cam1_person091.png\": \"person091\",\n",
        "    \"cam1_person092.png\": \"person092\",\n",
        "    \"cam1_person093.png\": \"person093\",\n",
        "    \"cam1_person094.png\": \"person094\",\n",
        "    \"cam1_person095.png\": \"person095\",\n",
        "    \"cam1_person096.png\": \"person096\",\n",
        "    \"cam1_person097.png\": \"person097\",\n",
        "    \"cam1_person098.png\": \"person098\",\n",
        "    \"cam1_person099.png\": \"person099\",\n",
        "    \"cam1_person100.png\": \"person100\",\n",
        "    \"cam1_person102.png\": \"person102\",\n",
        "    \"cam1_person103.png\": \"person103\",\n",
        "    \"cam1_person104.png\": \"person104\",\n",
        "    \"cam1_person105.png\": \"person105\",\n",
        "    \"cam1_person107.png\": \"person107\",\n",
        "    \"cam1_person108.png\": \"person108\",\n",
        "    \"cam1_person109.png\": \"person109\",\n",
        "    \"cam1_person113.png\": \"person113\",\n",
        "    \"cam1_person114.png\": \"person114\",\n",
        "    \"cam1_person117.png\": \"person117\",\n",
        "    \"cam1_person118.png\": \"person118\",\n",
        "    \"cam1_person119.png\": \"person119\",\n",
        "    \"cam1_person120.png\": \"person120\",\n",
        "    \"cam1_person121.png\": \"person121\",\n",
        "    \"cam1_person123.png\": \"person123\",\n",
        "    \"cam1_person124.png\": \"person124\",\n",
        "    \"cam1_person128.png\": \"person128\",\n",
        "    \"cam1_person129.png\": \"person129\",\n",
        "    \"cam1_person130.png\": \"person130\",\n",
        "    \"cam1_person131.png\": \"person131\",\n",
        "    \"cam1_person133.png\": \"person133\",\n",
        "    \"cam1_person134.png\": \"person134\",\n",
        "    \"cam1_person135.png\": \"person135\",\n",
        "    \"cam1_person136.png\": \"person136\",\n",
        "    \"cam1_person137.png\": \"person137\",\n",
        "    \"cam1_person142.png\": \"person142\",\n",
        "    \"cam1_person143.png\": \"person143\",\n",
        "    \"cam1_person145.png\": \"person145\",\n",
        "    \"cam1_person147.png\": \"person147\",\n",
        "    \"cam1_person150.png\": \"person150\",\n",
        "    \"cam1_person151.png\": \"person151\",\n",
        "    \"cam1_person152.png\": \"person152\",\n",
        "    \"cam1_person153.png\": \"person153\",\n",
        "    \"cam1_person154.png\": \"person154\",\n",
        "    \"cam1_person155.png\": \"person155\",\n",
        "    \"cam1_person157.png\": \"person157\",\n",
        "    \"cam1_person159.png\": \"person159\",\n",
        "    \"cam1_person160.png\": \"person160\",\n",
        "    \"cam1_person162.png\": \"person162\",\n",
        "    \"cam1_person163.png\": \"person163\",\n",
        "    \"cam1_person164.png\": \"person164\",\n",
        "    \"cam1_person165.png\": \"person165\",\n",
        "    \"cam1_person166.png\": \"person166\",\n",
        "    \"cam1_person167.png\": \"person167\",\n",
        "    \"cam1_person168.png\": \"person168\",\n",
        "    \"cam1_person169.png\": \"person169\",\n",
        "    \"cam1_person170.png\": \"person170\",\n",
        "    \"cam1_person171.png\": \"person171\",\n",
        "    \"cam1_person172.png\": \"person172\",\n",
        "    \"cam1_person173.png\": \"person173\",\n",
        "    \"cam1_person174.png\": \"person174\",\n",
        "    \"cam1_person175.png\": \"person175\",\n",
        "    \"cam1_person176.png\": \"person176\",\n",
        "    \"cam1_person177.png\": \"person177\",\n",
        "    \"cam1_person179.png\": \"person179\",\n",
        "    \"cam1_person181.png\": \"person181\",\n",
        "    \"cam1_person182.png\": \"person182\",\n",
        "    \"cam1_person183.png\": \"person183\",\n",
        "    \"cam1_person185.png\": \"person185\",\n",
        "    \"cam1_person187.png\": \"person187\",\n",
        "    \"cam1_person189.png\": \"person189\",\n",
        "    \"cam1_person191.png\": \"person191\",\n",
        "    \"cam1_person192.png\": \"person192\",\n",
        "    \"cam1_person193.png\": \"person193\",\n",
        "    \"cam1_person195.png\": \"person195\",\n",
        "    \"cam1_person196.png\": \"person196\",\n",
        "    \"cam1_person197.png\": \"person197\",\n",
        "    \"cam1_person198.png\": \"person198\",\n",
        "    \"cam1_person200.png\": \"person200\",\n",
        "    \"cam1_person202.png\": \"person202\",\n",
        "    \"cam1_person203.png\": \"person203\",\n",
        "    \"cam1_person204.png\": \"person204\",\n",
        "    \"cam1_person205.png\": \"person205\",\n",
        "    \"cam1_person206.png\": \"person206\",\n",
        "    \"cam1_person207.png\": \"person207\",\n",
        "    \"cam1_person208.png\": \"person208\",\n",
        "    \"cam1_person209.png\": \"person209\",\n",
        "    \"cam1_person210.png\": \"person210\",\n",
        "    \"cam1_person212.png\": \"person212\",\n",
        "    \"cam1_person213.png\": \"person213\",\n",
        "    \"cam1_person214.png\": \"person214\",\n",
        "    \"cam1_person216.png\": \"person216\",\n",
        "    \"cam1_person217.png\": \"person217\",\n",
        "    \"cam1_person218.png\": \"person218\",\n",
        "    \"cam1_person219.png\": \"person219\",\n",
        "    \"cam1_person220.png\": \"person220\",\n",
        "    \"cam1_person221.png\": \"person221\",\n",
        "    \"cam1_person222.png\": \"person222\",\n",
        "    \"cam1_person223.png\": \"person223\",\n",
        "    \"cam1_person224.png\": \"person224\",\n",
        "    \"cam1_person225.png\": \"person225\",\n",
        "    \"cam1_person226.png\": \"person226\",\n",
        "    \"cam1_person227.png\": \"person227\",\n",
        "    \"cam1_person228.png\": \"person228\",\n",
        "    \"cam1_person229.png\": \"person229\",\n",
        "    \"cam1_person230.png\": \"person230\",\n",
        "    \"cam1_person231.png\": \"person231\",\n",
        "    \"cam1_person232.png\": \"person232\",\n",
        "    \"cam1_person233.png\": \"person233\",\n",
        "    \"cam1_person234.png\": \"person234\",\n",
        "    \"cam1_person235.png\": \"person235\",\n",
        "    \"cam1_person237.png\": \"person237\",\n",
        "    \"cam1_person238.png\": \"person238\",\n",
        "    \"cam1_person239.png\": \"person239\",\n",
        "    \"cam1_person240.png\": \"person240\",\n",
        "    \"cam1_person241.png\": \"person241\",\n",
        "    \"cam1_person242.png\": \"person242\",\n",
        "    \"cam1_person243.png\": \"person243\",\n",
        "    \"cam1_person244.png\": \"person244\",\n",
        "    \"cam1_person245.png\": \"person245\",\n",
        "    \"cam1_person246.png\": \"person246\",\n",
        "    \"cam1_person251.png\": \"person251\",\n",
        "    \"cam1_person252.png\": \"person252\",\n",
        "    \"cam1_person253.png\": \"person253\",\n",
        "    \"cam1_person254.png\": \"person254\",\n",
        "    \"cam1_person255.png\": \"person255\",\n",
        "    \"cam1_person256.png\": \"person256\",\n",
        "    \"cam1_person257.png\": \"person257\",\n",
        "    \"cam1_person258.png\": \"person258\",\n",
        "    \"cam1_person260.png\": \"person260\",\n",
        "    \"cam1_person261.png\": \"person261\",\n",
        "    \"cam1_person262.png\": \"person262\",\n",
        "    \"cam1_person264.png\": \"person264\",\n",
        "    \"cam1_person265.png\": \"person265\",\n",
        "    \"cam1_person266.png\": \"person266\",\n",
        "    \"cam1_person268.png\": \"person268\",\n",
        "    \"cam1_person269.png\": \"person269\",\n",
        "    \"cam1_person270.png\": \"person270\",\n",
        "    \"cam1_person271.png\": \"person271\",\n",
        "    \"cam1_person272.png\": \"person272\",\n",
        "    \"cam1_person273.png\": \"person273\",\n",
        "    \"cam1_person274.png\": \"person274\",\n",
        "    \"cam1_person275.png\": \"person275\",\n",
        "    \"cam1_person276.png\": \"person276\",\n",
        "    \"cam1_person278.png\": \"person278\",\n",
        "    \"cam1_person279.png\": \"person279\",\n",
        "    \"cam1_person280.png\": \"person280\",\n",
        "    \"cam1_person281.png\": \"person281\",\n",
        "    \"cam1_person282.png\": \"person282\",\n",
        "    \"cam1_person284.png\": \"person284\",\n",
        "    \"cam1_person285.png\": \"person285\",\n",
        "    \"cam1_person286.png\": \"person286\",\n",
        "    \"cam1_person287.png\": \"person287\",\n",
        "    \"cam1_person288.png\": \"person288\",\n",
        "    \"cam1_person289.png\": \"person289\",\n",
        "    \"cam1_person290.png\": \"person290\",\n",
        "    \"cam1_person293.png\": \"person293\",\n",
        "    \"cam1_person294.png\": \"person294\",\n",
        "    \"cam1_person295.png\": \"person295\",\n",
        "    \"cam1_person296.png\": \"person296\",\n",
        "    \"cam1_person297.png\": \"person297\",\n",
        "    \"cam1_person298.png\": \"person298\",\n",
        "    \"cam1_person299.png\": \"person299\",\n",
        "    \"cam1_person300.png\": \"person300\",\n",
        "    \"cam1_person301.png\": \"person301\",\n",
        "    \"cam1_person302.png\": \"person302\",\n",
        "    \"cam1_person303.png\": \"person303\",\n",
        "    \"cam1_person304.png\": \"person304\",\n",
        "    \"cam1_person307.png\": \"person307\",\n",
        "    \"cam1_person308.png\": \"person308\",\n",
        "    \"cam1_person309.png\": \"person309\",\n",
        "    \"cam1_person310.png\": \"person310\",\n",
        "    \"cam1_person311.png\": \"person311\",\n",
        "    \"cam1_person312.png\": \"person312\",\n",
        "    \"cam1_person313.png\": \"person313\",\n",
        "    \"cam1_person314.png\": \"person314\",\n",
        "    \"cam1_person315.png\": \"person315\",\n",
        "    \"cam1_person316.png\": \"person316\",\n",
        "    \"cam1_person317.png\": \"person317\",\n",
        "    \"cam1_person319.png\": \"person319\"\n",
        "\n",
        "}\n",
        "mean_ap = compute_map(\"/content/Dataset/query\", query_labels, features_dict_2, resnet50, transform, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7xR78ClCm5a"
      },
      "outputs": [],
      "source": [
        "# Accuracy values\n",
        "rank1_acc = 75.61\n",
        "rank5_acc = 90.24\n",
        "rank10_acc = 93.90\n",
        "# Ranks\n",
        "ranks = [1, 5, 10]\n",
        "accuracies = [rank1_acc, rank5_acc, rank10_acc]\n",
        "\n",
        "# Plot accuracy trends\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(ranks, accuracies, marker='o', linestyle='-', color='b', label='Accuracy')\n",
        "plt.xlabel('Rank')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Rank-wise Accuracy Curve')\n",
        "plt.xticks(ranks)\n",
        "plt.yticks(range(50, 101, 5))\n",
        "plt.ylim(50, 105)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZiD-FOmNhbG"
      },
      "outputs": [],
      "source": [
        "#create the dataframe and show rank1,rank5,rank10 and map score of swin transformer,resnet50 and vgg19 models\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'Model': ['Swin Transformer', 'ResNet50'],\n",
        "    'Rank 1': [95.12, 75.61],\n",
        "    'Rank 5': [97.97, 90.24],\n",
        "    'Rank 10': [99.19, 93.90],\n",
        "    'mAP Score': [96.27, 81.83]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsLxZtn0Rw_E"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "data = {\n",
        "    'Model': ['Swin Transformer', 'ResNet50'],\n",
        "    'Rank 1': [95.12, 75.61],\n",
        "    'Rank 5': [97.97, 90.24],\n",
        "    'Rank 10': [99.19, 93.90],\n",
        "    'mAP Score': [96.27, 81.83]\n",
        "}\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Plotting the data\n",
        "models = df['Model']\n",
        "rank1 = df['Rank 1']\n",
        "rank5 = df['Rank 5']\n",
        "rank10 = df['Rank 10']\n",
        "map_scores = df['mAP Score']\n",
        "\n",
        "x = range(len(models))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.plot(x, rank1, marker='o', label='Rank 1')\n",
        "plt.plot(x, rank5, marker='s', label='Rank 5')\n",
        "plt.plot(x, rank10, marker='^', label='Rank 10')\n",
        "plt.plot(x, map_scores, marker='x', label='mAP Score')\n",
        "\n",
        "plt.xticks(x, models, rotation=45, ha='right')  # Rotate model names for better readability\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Accuracy (%) / mAP Score')\n",
        "plt.title('Rank-wise Accuracy and mAP Score Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()  # Adjust layout to prevent labels from overlapping\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AwvLY3xR9Ch"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
