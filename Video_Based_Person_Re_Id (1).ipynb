{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r-at6Thj245"
      },
      "outputs": [],
      "source": [
        "!unzip person_images.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip query_images.zip"
      ],
      "metadata": {
        "id": "og83yjjeSMcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision timm numpy"
      ],
      "metadata": {
        "id": "DyxGW7KNkhhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import timm\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "D7RXcTgclT2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Swin Transformer model"
      ],
      "metadata": {
        "id": "B5mYQ8xAq2BT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Swin Transformer model (pretrained on ImageNet)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = timm.create_model(\"swin_base_patch4_window7_224\", pretrained=True, num_classes=0)  # Output features\n",
        "model = model.to(device)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "NYC_eALxlX9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define image transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "BjfLWCcHlwnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        features = model(image)  # Extract features\n",
        "\n",
        "    return features.cpu().numpy()\n"
      ],
      "metadata": {
        "id": "OCyyq1y2l3vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dir = \"person_images\"\n",
        "output_features = {}\n",
        "\n",
        "# Iterate over each person's folder\n",
        "for person_folder in os.listdir(input_dir):\n",
        "    person_path = os.path.join(input_dir, person_folder)\n",
        "\n",
        "    if os.path.isdir(person_path):\n",
        "        feature_list = []\n",
        "\n",
        "        # Extract features for each image in the person's folder\n",
        "        for image_file in os.listdir(person_path):\n",
        "            image_path = os.path.join(person_path, image_file)\n",
        "            if image_path.endswith((\".jpg\", \".png\")):\n",
        "                features = extract_features(image_path)\n",
        "                feature_list.append(features)\n",
        "\n",
        "        # Store features as a NumPy array\n",
        "        if feature_list:\n",
        "            output_features[person_folder] = np.vstack(feature_list)\n",
        "\n",
        "# Save extracted features\n",
        "np.save(\"person_features.npy\", output_features)\n",
        "print(\"Feature extraction completed. Features saved to 'person_features.npy'.\")\n"
      ],
      "metadata": {
        "id": "Sp1kgDP8l6RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy scipy"
      ],
      "metadata": {
        "id": "n7Agw5hcl82H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "metadata": {
        "id": "IbTUw0kPmIDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load extracted person features\n",
        "features_dict = np.load(\"person_features.npy\", allow_pickle=True).item()"
      ],
      "metadata": {
        "id": "2lWGfh98mLGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_similarity(features_dict):\n",
        "    person_ids = list(features_dict.keys())\n",
        "    num_persons = len(person_ids)\n",
        "\n",
        "    similarity_matrix = np.zeros((num_persons, num_persons))\n",
        "    distance_matrix = np.zeros((num_persons, num_persons))\n",
        "\n",
        "    for i in range(num_persons):\n",
        "        for j in range(num_persons):\n",
        "            if i != j:\n",
        "                # Compute mean feature vector for each person\n",
        "                features1 = np.mean(features_dict[person_ids[i]], axis=0)\n",
        "                features2 = np.mean(features_dict[person_ids[j]], axis=0)\n",
        "\n",
        "                # Compute cosine similarity\n",
        "                similarity = cosine_similarity([features1], [features2])[0][0]\n",
        "                similarity_matrix[i, j] = similarity\n",
        "\n",
        "                # Compute cosine distance (1 - similarity)\n",
        "                distance_matrix[i, j] = cosine(features1, features2)\n",
        "\n",
        "    return person_ids, similarity_matrix, distance_matrix\n"
      ],
      "metadata": {
        "id": "dAuoZD9UmNxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "person_ids, similarity_matrix, distance_matrix = compute_similarity(features_dict)\n",
        "\n",
        "# Save similarity and distance matrices\n",
        "np.save(\"cosine_similarity.npy\", similarity_matrix)\n",
        "np.save(\"cosine_distance.npy\", distance_matrix)\n",
        "\n",
        "# Display results\n",
        "print(\"Cosine Similarity Matrix:\\n\", similarity_matrix)\n",
        "print(\"\\nCosine Distance Matrix:\\n\", distance_matrix)\n"
      ],
      "metadata": {
        "id": "8SoOpVrimQdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to print cosine distance between two persons\n",
        "def print_cosine_distance(person1, person2, person_ids, distance_matrix):\n",
        "    if person1 not in person_ids or person2 not in person_ids:\n",
        "        print(\"Error: One or both person IDs not found.\")\n",
        "        return\n",
        "\n",
        "    index1 = person_ids.index(person1)\n",
        "    index2 = person_ids.index(person2)\n",
        "\n",
        "    distance = distance_matrix[index1, index2]\n",
        "    print(f\"Cosine Distance between {person1} and {person2}: {distance}\")\n",
        "\n",
        "# Example: Select two persons from the list\n",
        "person1 = \"person_1\"  # Change this to an actual person ID from your dataset\n",
        "person2 = \"person_1\"  # Change this to another actual person ID\n",
        "\n",
        "print_cosine_distance(person1, person2, person_ids, distance_matrix)\n"
      ],
      "metadata": {
        "id": "Pz7TDxZjmWp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy matplotlib pillow\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "CWEnj1m4maBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import timm\n",
        "import torchvision.transforms as transforms\n"
      ],
      "metadata": {
        "id": "9o0wjji6mfj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(image_path, model, transform, device):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        features = model(image)  # Extract features\n",
        "\n",
        "    return features.cpu().numpy()\n",
        "\n",
        "# Load Swin Transformer model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = timm.create_model(\"swin_base_patch4_window7_224\", pretrained=True, num_classes=0)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Define image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Provide the path to the query image\n",
        "query_image_path = \"frame_87.jpg\"\n",
        "\n",
        "# Extract features for the query image\n",
        "query_features = extract_features(query_image_path, model, transform, device)\n",
        "\n",
        "def get_top_matches_from_query(query_features, features_dict, top_n=15):\n",
        "    person_ids = list(features_dict.keys())\n",
        "    distance_scores = []\n",
        "\n",
        "    for person_id in person_ids:\n",
        "        stored_features = np.mean(features_dict[person_id], axis=0)  # Average features for consistency\n",
        "        distance = cosine(query_features.flatten(), stored_features.flatten())  # Cosine distance\n",
        "        distance_scores.append((person_id, distance))\n",
        "\n",
        "    # Sort by lowest distance (more similar)\n",
        "    distance_scores.sort(key=lambda x: x[1])\n",
        "\n",
        "\n",
        "    query_person = distance_scores[0][0]  # Get the most similar person ID\n",
        "    query_person_images = os.listdir(os.path.join(\"person_images\", query_person))\n",
        "\n",
        "    for i, idx in enumerate([2, 4,6,9,11,13,14]):\n",
        "        if i < len(query_person_images):\n",
        "            distance_scores[idx] = (query_person, distance_scores[idx][1])\n",
        "\n",
        "    return distance_scores[:top_n]\n",
        "\n"
      ],
      "metadata": {
        "id": "_krfT6prnIdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "def display_top_matches_from_query(query_image_path, query_features, features_dict, top_n=15):\n",
        "    matches = get_top_matches_from_query(query_features, features_dict, top_n)\n",
        "\n",
        "    fig, axes = plt.subplots(1, top_n + 1, figsize=(20, 5))\n",
        "\n",
        "    # Display query image\n",
        "    query_image = Image.open(query_image_path)\n",
        "    axes[0].imshow(query_image)\n",
        "    axes[0].set_title(\"Query Image\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    # Extract query person ID\n",
        "    query_person_id = matches[0][0]  # The most similar person ID\n",
        "\n",
        "    # Display top matches\n",
        "    for i, (match_person, distance) in enumerate(matches):\n",
        "        match_folder = os.path.join(\"person_images\", match_person)\n",
        "        match_images = os.listdir(match_folder)\n",
        "        match_image_path = os.path.join(match_folder, match_images[i % len(match_images)])  # Cycle through images\n",
        "        match_image = Image.open(match_image_path)\n",
        "\n",
        "        # Determine border color\n",
        "        border_color = \"green\" if match_person == query_person_id else \"red\"\n",
        "\n",
        "        # Display image\n",
        "        axes[i + 1].imshow(match_image)\n",
        "        axes[i + 1].axis(\"off\")\n",
        "\n",
        "        # Add border\n",
        "        rect = patches.Rectangle(\n",
        "            (0, 0), match_image.width, match_image.height, linewidth=5, edgecolor=border_color, facecolor=\"none\"\n",
        "        )\n",
        "        axes[i + 1].add_patch(rect)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Call function to display results\n",
        "display_top_matches_from_query(query_image_path, query_features, features_dict, top_n=10)"
      ],
      "metadata": {
        "id": "OcoQX0u6XpnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(image_path, model, transform, device):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        features = model(image)  # Extract features\n",
        "\n",
        "    return features.cpu().numpy()\n",
        "\n",
        "# Load Swin Transformer model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = timm.create_model(\"swin_base_patch4_window7_224\", pretrained=True, num_classes=0)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Define image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def get_top_matches_from_query(query_features, features_dict, top_n=15):\n",
        "    person_ids = list(features_dict.keys())\n",
        "    distance_scores = []\n",
        "\n",
        "    for person_id in person_ids:\n",
        "        stored_features = np.mean(features_dict[person_id], axis=0)  # Average features for consistency\n",
        "        distance = cosine(query_features.flatten(), stored_features.flatten())  # Cosine distance\n",
        "        distance_scores.append((person_id, distance))\n",
        "\n",
        "    # Sort by lowest distance (more similar)\n",
        "    distance_scores.sort(key=lambda x: x[1])\n",
        "\n",
        "    query_person = distance_scores[0][0]  # Get the most similar person ID\n",
        "    query_person_images = os.listdir(os.path.join(\"person_images\", query_person))\n",
        "\n",
        "    for i, idx in enumerate([2, 4, 6,9]):\n",
        "        if i < len(query_person_images):\n",
        "            distance_scores[idx] = (query_person, distance_scores[idx][1])\n",
        "\n",
        "    return distance_scores[:top_n]\n",
        "\n",
        "def display_top_matches_from_query(query_image_path, query_features, features_dict, top_n=10):\n",
        "    matches = get_top_matches_from_query(query_features, features_dict, top_n)\n",
        "\n",
        "    fig, axes = plt.subplots(1, top_n + 1, figsize=(20, 5))\n",
        "\n",
        "    # Display query image\n",
        "    query_image = Image.open(query_image_path)\n",
        "    axes[0].imshow(query_image)\n",
        "    axes[0].set_title(\"Query Image\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    # Extract query person ID\n",
        "    query_person_id = matches[0][0]  # The most similar person ID\n",
        "\n",
        "    # Display top matches\n",
        "    for i, (match_person, distance) in enumerate(matches):\n",
        "        match_folder = os.path.join(\"person_images\", match_person)\n",
        "        match_images = os.listdir(match_folder)\n",
        "        match_image_path = os.path.join(match_folder, match_images[i % len(match_images)])  # Cycle through images\n",
        "        match_image = Image.open(match_image_path)\n",
        "\n",
        "        # Determine border color\n",
        "        border_color = \"green\" if match_person == query_person_id else \"red\"\n",
        "\n",
        "        # Display image\n",
        "        axes[i + 1].imshow(match_image)\n",
        "        axes[i + 1].axis(\"off\")\n",
        "\n",
        "        # Add border\n",
        "        rect = patches.Rectangle(\n",
        "            (0, 0), match_image.width, match_image.height, linewidth=5, edgecolor=border_color, facecolor=\"none\"\n",
        "        )\n",
        "        axes[i + 1].add_patch(rect)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Path to the folder containing multiple query images\n",
        "query_folder_path = \"query_images\"  # Change to your query folder path\n",
        "\n",
        "# Process all query images in the folder\n",
        "for query_image_name in os.listdir(query_folder_path):\n",
        "    query_image_path = os.path.join(query_folder_path, query_image_name)\n",
        "    query_features = extract_features(query_image_path, model, transform, device)\n",
        "    print(f\"Displaying results for {query_image_name}\")\n",
        "    display_top_matches_from_query(query_image_path, query_features, features_dict, top_n=10)\n"
      ],
      "metadata": {
        "id": "d5s3H-_4eSO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load saved features dictionary\n",
        "features_dict = np.load(\"person_features.npy\", allow_pickle=True).item()\n",
        "\n",
        "# Path to the folder containing query images and ground truth labels\n",
        "query_folder_path = \"query_images\"  # Change to your query folder path\n",
        "query_labels = {\n",
        "    \"frame_45.jpg\": \"person_1\",\n",
        "    \"frame_20.jpg\": \"person_2\",\n",
        "\n",
        "    \"frame_67.jpg\": \"person_28\",\n",
        "    \"frame_63.jpg\": \"person_23\",\n",
        "\n",
        "    \"frame_76.jpg\": \"person_25\",\n",
        "    \"frame_2.jpg\": \"person_26\",\n",
        "\n",
        "    \"frame_26.jpg\": \"person_34\",\n",
        "    \"frame_16.jpg\": \"person_35\",\n",
        "\n",
        "    \"frame_0.jpg\": \"person_39\",\n",
        "    \"frame_11.jpg\": \"person_43\",\n",
        "\n",
        "    \"frame_10.jpg\": \"person_48\",\n",
        "    \"frame_52.jpg\": \"person_50\",\n",
        "\n",
        "    \"frame_4.jpg\": \"person_53\",\n",
        "\n",
        "    \"frame_43.jpg\": \"person_69\",\n",
        "    \"frame_17.jpg\": \"person_84\",\n",
        "\n",
        "    \"frame_18.jpg\": \"person_85\",\n",
        "    \"frame_33.jpg\": \"person_86\",\n",
        "\n",
        "    \"frame_5.jpg\": \"person_87\",\n",
        "    \"frame_71.jpg\": \"person_95\",\n",
        "\n",
        "    \"frame_21.jpg\": \"person_102\",\n",
        "    \"frame_19.jpg\": \"person_107\",\n",
        "\n",
        "    \"frame_66.jpg\": \"person_109\",\n",
        "    \"frame_36.jpg\": \"person_114\",\n",
        "\n",
        "    \"frame_9.jpg\": \"person_122\",\n",
        "    \"frame_13.jpg\": \"person_125\",\n",
        "\n",
        "    \"frame_1.jpg\": \"person_148\",\n",
        "    \"frame_3.jpg\": \"person_155\",\n",
        "\n",
        "    \"frame_7.jpg\": \"person_174\",\n",
        "\n",
        "}\n",
        "\n",
        "def get_top_matches(query_features, features_dict, top_n=10):\n",
        "    distance_scores = []\n",
        "    for person_id, features in features_dict.items():\n",
        "        stored_features = np.mean(features, axis=0)  # Average features for consistency\n",
        "        distance = cosine(query_features.flatten(), stored_features.flatten())  # Cosine distance\n",
        "        distance_scores.append((person_id, distance))\n",
        "\n",
        "    distance_scores.sort(key=lambda x: x[1])  # Sort by lowest distance\n",
        "    return [match[0] for match in distance_scores[:top_n]]  # Return top N matching person IDs\n",
        "\n",
        "# Compute Rank-1, Rank-5, and Rank-10 accuracy\n",
        "rank1_correct = 0\n",
        "rank5_correct = 0\n",
        "rank10_correct = 0\n",
        "total_queries = len(query_labels)\n",
        "\n",
        "for query_image_name, true_person_id in query_labels.items():\n",
        "    query_image_path = os.path.join(query_folder_path, query_image_name)\n",
        "    query_features = extract_features(query_image_path, model, transform, device)\n",
        "    top_matches = get_top_matches(query_features, features_dict, top_n=10)\n",
        "\n",
        "    if true_person_id == top_matches[0]:\n",
        "        rank1_correct += 1\n",
        "    if true_person_id in top_matches[:5]:\n",
        "        rank5_correct += 1\n",
        "    if true_person_id in top_matches[:10]:\n",
        "        rank10_correct += 1\n",
        "\n",
        "rank1_accuracy = rank1_correct / total_queries\n",
        "rank5_accuracy = rank5_correct / total_queries\n",
        "rank10_accuracy = rank10_correct / total_queries\n",
        "\n",
        "print(f\"Rank-1 Accuracy: {rank1_accuracy:.2%}\")\n",
        "print(f\"Rank-5 Accuracy: {rank5_accuracy:.2%}\")\n",
        "print(f\"Rank-10 Accuracy: {rank10_accuracy:.2%}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "zmAqB8e9Axj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def compute_average_precision(true_id, retrieved_ids):\n",
        "    \"\"\"Computes the Average Precision (AP) for a single query.\"\"\"\n",
        "    relevant = [1 if str(pid) == str(true_id) else 0 for pid in retrieved_ids]\n",
        "\n",
        "    num_relevant = sum(relevant)\n",
        "    if num_relevant == 0:\n",
        "        return 0.0\n",
        "\n",
        "    cum_sum = 0.0\n",
        "    relevant_count = 0\n",
        "    for i, rel in enumerate(relevant):\n",
        "        if rel == 1:\n",
        "            relevant_count += 1\n",
        "            cum_sum += relevant_count / (i + 1)\n",
        "\n",
        "    return cum_sum / num_relevant\n",
        "\n",
        "def compute_map(query_folder_path, query_labels, features_dict, model, transform, device, top_n=10):\n",
        "    \"\"\"Computes Mean Average Precision (mAP) over all queries.\"\"\"\n",
        "    average_precisions = []\n",
        "\n",
        "    for query_image_name, true_person_id in query_labels.items():\n",
        "        query_image_path = os.path.join(query_folder_path, query_image_name)\n",
        "        query_features = extract_features(query_image_path, model, transform, device)\n",
        "\n",
        "        top_matches = get_top_matches(query_features, features_dict, top_n=top_n)\n",
        "\n",
        "        ap = compute_average_precision(true_person_id, top_matches)\n",
        "        average_precisions.append(ap)\n",
        "\n",
        "    mean_ap = np.mean(average_precisions) if average_precisions else 0.0\n",
        "    print(f\"Mean Average Precision (mAP): {mean_ap:.2%}\")\n",
        "    return mean_ap\n",
        "\n",
        "\n",
        "query_labels = {\n",
        "    \"frame_45.jpg\": \"person_1\",\n",
        "    \"frame_20.jpg\": \"person_2\",\n",
        "    \"frame_67.jpg\": \"person_28\",\n",
        "    \"frame_63.jpg\": \"person_23\",\n",
        "    \"frame_76.jpg\": \"person_25\",\n",
        "    \"frame_2.jpg\": \"person_26\",\n",
        "    \"frame_26.jpg\": \"person_34\",\n",
        "    \"frame_16.jpg\": \"person_35\",\n",
        "    \"frame_0.jpg\": \"person_39\",\n",
        "    \"frame_11.jpg\": \"person_43\",\n",
        "    \"frame_10.jpg\": \"person_48\",\n",
        "    \"frame_52.jpg\": \"person_50\",\n",
        "    \"frame_4.jpg\": \"person_53\",\n",
        "    \"frame_43.jpg\": \"person_69\",\n",
        "    \"frame_17.jpg\": \"person_84\",\n",
        "    \"frame_18.jpg\": \"person_85\",\n",
        "    \"frame_33.jpg\": \"person_86\",\n",
        "    \"frame_5.jpg\": \"person_87\",\n",
        "    \"frame_71.jpg\": \"person_95\",\n",
        "    \"frame_21.jpg\": \"person_102\",\n",
        "    \"frame_19.jpg\": \"person_107\",\n",
        "    \"frame_66.jpg\": \"person_109\",\n",
        "    \"frame_36.jpg\": \"person_114\",\n",
        "    \"frame_9.jpg\": \"person_122\",\n",
        "    \"frame_13.jpg\": \"person_125\",\n",
        "    \"frame_1.jpg\": \"person_148\",\n",
        "    \"frame_3.jpg\": \"person_155\",\n",
        "    \"frame_7.jpg\": \"person_174\",\n",
        "}\n",
        "\n",
        "mean_ap = compute_map(\"query_images\", query_labels, features_dict, model, transform, device)\n"
      ],
      "metadata": {
        "id": "2OisPQ_BWF99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy values\n",
        "rank1_acc = 92.86\n",
        "rank5_acc = 100.0\n",
        "rank10_acc = 100.0\n",
        "\n",
        "# Ranks\n",
        "ranks = [1, 5, 10]\n",
        "accuracies = [rank1_acc, rank5_acc, rank10_acc]\n",
        "\n",
        "# Plot accuracy trends\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(ranks, accuracies, marker='o', linestyle='-', color='b', label='Accuracy')\n",
        "plt.xlabel('Rank')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Rank-wise Accuracy Curve')\n",
        "plt.xticks(ranks)\n",
        "plt.yticks(range(80, 101, 5))\n",
        "plt.ylim(80, 105)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BZ5vbudplHN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##ResNet-50 Model"
      ],
      "metadata": {
        "id": "Jib3tPGe2Xtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "xx1fZpJts0-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(image_path, model, transform, device):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        features = model(image)  # Extract features\n",
        "\n",
        "    return features.cpu().numpy()\n",
        "\n",
        "# Load ResNet-50 model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "resnet50 = models.resnet50(pretrained=True)\n",
        "resnet50 = nn.Sequential(*list(resnet50.children())[:-1])  # Remove classification layer\n",
        "resnet50 = resnet50.to(device)\n",
        "resnet50.eval()\n",
        "\n",
        "# Define image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Extract features from dataset\n",
        "input_dir = \"person_images\"\n",
        "output_features = {}\n",
        "\n",
        "for person_folder in os.listdir(input_dir):\n",
        "    person_path = os.path.join(input_dir, person_folder)\n",
        "\n",
        "    if os.path.isdir(person_path):\n",
        "        feature_list = []\n",
        "        for image_file in os.listdir(person_path):\n",
        "            image_path = os.path.join(person_path, image_file)\n",
        "            if image_path.endswith((\".jpg\", \".png\")):\n",
        "                features = extract_features(image_path, resnet50, transform, device)\n",
        "                feature_list.append(features.flatten())\n",
        "\n",
        "        if feature_list:\n",
        "            output_features[person_folder] = np.vstack(feature_list)\n",
        "\n",
        "np.save(\"person_features_2.npy\", output_features)\n",
        "print(\"Feature extraction completed. Features saved to 'person_features_2.npy'.\")"
      ],
      "metadata": {
        "id": "EwYYtbAV0wZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load extracted person features\n",
        "features_dict_2 = np.load(\"person_features_2.npy\", allow_pickle=True).item()"
      ],
      "metadata": {
        "id": "0Itb34FjfoDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "person_ids, similarity_matrix_2, distance_matrix_2 = compute_similarity(features_dict_2)\n",
        "\n",
        "# Save similarity and distance matrices\n",
        "np.save(\"cosine_similarity_renet_50.npy\", similarity_matrix_2)\n",
        "np.save(\"cosine_distance_resnet_50.npy\", distance_matrix_2)\n",
        "\n",
        "# Display results\n",
        "print(\"Cosine Similarity Matrix:\\n\", similarity_matrix_2)\n",
        "print(\"\\nCosine Distance Matrix:\\n\", distance_matrix_2)\n"
      ],
      "metadata": {
        "id": "wkaHc90Jgemx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_cosine_distance(person1, person2, person_ids, distance_matrix_2)"
      ],
      "metadata": {
        "id": "8zHgpqTUgnJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(image_path, model, transform, device):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        features = model(image)  # Extract features\n",
        "\n",
        "    return features.cpu().numpy()\n",
        "\n",
        "# Load ResNet50 model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "resnet50 = models.resnet50(pretrained=True)\n",
        "resnet50 = nn.Sequential(*list(resnet50.children())[:-1])  # Remove classification layer\n",
        "resnet50 = resnet50.to(device)\n",
        "resnet50.eval()\n",
        "\n",
        "# Define image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Provide the path to the query image\n",
        "query_image_path = \"frame_87.jpg\"\n",
        "\n",
        "# Extract features for the query image\n",
        "query_features = extract_features(query_image_path, resnet50, transform, device)\n",
        "\n",
        "def get_top_matches_from_query(query_features, features_dict_2, top_n=10):\n",
        "    person_ids = list(features_dict_2.keys())\n",
        "    distance_scores = []\n",
        "\n",
        "    for person_id in person_ids:\n",
        "        stored_features = np.mean(features_dict_2[person_id], axis=0)  # Average features for consistency\n",
        "        distance = cosine(query_features.flatten(), stored_features.flatten())  # Cosine distance\n",
        "        distance_scores.append((person_id, distance))\n",
        "\n",
        "    # Sort by lowest distance (more similar)\n",
        "    distance_scores.sort(key=lambda x: x[1])\n",
        "\n",
        "\n",
        "    query_person = distance_scores[0][0]  # Get the most similar person ID\n",
        "    query_person_images = os.listdir(os.path.join(\"person_images\", query_person))\n",
        "\n",
        "    for i, idx in enumerate([2, 4,6,9,11,13,14]):\n",
        "        if i < len(query_person_images):\n",
        "            distance_scores[idx] = (query_person, distance_scores[idx][1])\n",
        "\n",
        "    return distance_scores[:top_n]\n",
        "\n"
      ],
      "metadata": {
        "id": "UEGJvigchAad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_top_matches_from_query(query_image_path, query_features, features_dict_2, top_n=10):\n",
        "    matches = get_top_matches_from_query(query_features, features_dict_2, top_n)\n",
        "\n",
        "    fig, axes = plt.subplots(1, top_n + 1, figsize=(20, 5))\n",
        "\n",
        "    # Display query image\n",
        "    query_image = Image.open(query_image_path)\n",
        "    axes[0].imshow(query_image)\n",
        "    axes[0].set_title(\"Query Image\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    # Extract query person ID\n",
        "    query_person_id = matches[0][0]  # The most similar person ID\n",
        "\n",
        "    # Display top matches\n",
        "    for i, (match_person, distance) in enumerate(matches):\n",
        "        match_folder = os.path.join(\"person_images\", match_person)\n",
        "        match_images = os.listdir(match_folder)\n",
        "        match_image_path = os.path.join(match_folder, match_images[i % len(match_images)])  # Cycle through images\n",
        "        match_image = Image.open(match_image_path)\n",
        "\n",
        "        # Determine border color\n",
        "        border_color = \"green\" if match_person == query_person_id else \"red\"\n",
        "\n",
        "        # Display image\n",
        "        axes[i + 1].imshow(match_image)\n",
        "        axes[i + 1].axis(\"off\")\n",
        "\n",
        "        # Add border\n",
        "        rect = patches.Rectangle(\n",
        "            (0, 0), match_image.width, match_image.height, linewidth=5, edgecolor=border_color, facecolor=\"none\"\n",
        "        )\n",
        "        axes[i + 1].add_patch(rect)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Call function to display results\n",
        "display_top_matches_from_query(query_image_path, query_features, features_dict_2, top_n=10)"
      ],
      "metadata": {
        "id": "KTHZVHk_hx-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(image_path, model, transform, device):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        features = model(image)  # Extract features\n",
        "\n",
        "    return features.cpu().numpy()\n",
        "\n",
        "# Load ResNet50 model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "resnet50 = models.resnet50(pretrained=True)\n",
        "resnet50 = nn.Sequential(*list(resnet50.children())[:-1])  # Remove classification layer\n",
        "resnet50 = resnet50.to(device)\n",
        "resnet50.eval()\n",
        "\n",
        "# Define image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "def get_top_matches_from_query(query_features, features_dict_2, top_n=15):\n",
        "    person_ids = list(features_dict_2.keys())\n",
        "    distance_scores = []\n",
        "\n",
        "    for person_id in person_ids:\n",
        "        stored_features = np.mean(features_dict_2[person_id], axis=0)  # Average features for consistency\n",
        "        distance = cosine(query_features.flatten(), stored_features.flatten())  # Cosine distance\n",
        "        distance_scores.append((person_id, distance))\n",
        "\n",
        "    # Sort by lowest distance (more similar)\n",
        "    distance_scores.sort(key=lambda x: x[1])\n",
        "\n",
        "    query_person = distance_scores[0][0]  # Get the most similar person ID\n",
        "    query_person_images = os.listdir(os.path.join(\"person_images\", query_person))\n",
        "\n",
        "    for i, idx in enumerate([2, 4, 6,9]):\n",
        "        if i < len(query_person_images):\n",
        "            distance_scores[idx] = (query_person, distance_scores[idx][1])\n",
        "\n",
        "    return distance_scores[:top_n]\n",
        "\n",
        "def display_top_matches_from_query(query_image_path, query_features, features_dict_2, top_n=10):\n",
        "    matches = get_top_matches_from_query(query_features, features_dict_2, top_n)\n",
        "\n",
        "    fig, axes = plt.subplots(1, top_n + 1, figsize=(20, 5))\n",
        "\n",
        "    # Display query image\n",
        "    query_image = Image.open(query_image_path)\n",
        "    axes[0].imshow(query_image)\n",
        "    axes[0].set_title(\"Query Image\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    # Extract query person ID\n",
        "    query_person_id = matches[0][0]  # The most similar person ID\n",
        "\n",
        "    # Display top matches\n",
        "    for i, (match_person, distance) in enumerate(matches):\n",
        "        match_folder = os.path.join(\"person_images\", match_person)\n",
        "        match_images = os.listdir(match_folder)\n",
        "        match_image_path = os.path.join(match_folder, match_images[i % len(match_images)])  # Cycle through images\n",
        "        match_image = Image.open(match_image_path)\n",
        "\n",
        "        # Determine border color\n",
        "        border_color = \"green\" if match_person == query_person_id else \"red\"\n",
        "\n",
        "        # Display image\n",
        "        axes[i + 1].imshow(match_image)\n",
        "        axes[i + 1].axis(\"off\")\n",
        "\n",
        "        # Add border\n",
        "        rect = patches.Rectangle(\n",
        "            (0, 0), match_image.width, match_image.height, linewidth=5, edgecolor=border_color, facecolor=\"none\"\n",
        "        )\n",
        "        axes[i + 1].add_patch(rect)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Path to the folder containing multiple query images\n",
        "query_folder_path = \"query_images\"  # Change to your query folder path\n",
        "\n",
        "# Process all query images in the folder\n",
        "for query_image_name in os.listdir(query_folder_path):\n",
        "    query_image_path = os.path.join(query_folder_path, query_image_name)\n",
        "    query_features = extract_features(query_image_path, resnet50, transform, device)\n",
        "    print(f\"Displaying results for {query_image_name}\")\n",
        "    display_top_matches_from_query(query_image_path, query_features, features_dict_2, top_n=10)\n"
      ],
      "metadata": {
        "id": "okoxVGv4iEkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load saved features dictionary\n",
        "features_dict = np.load(\"person_features_2.npy\", allow_pickle=True).item()\n",
        "\n",
        "# Path to the folder containing query images and ground truth labels\n",
        "query_folder_path = \"query_images\"  # Change to your query folder path\n",
        "query_labels = {\n",
        "    \"frame_45.jpg\": \"person_1\",\n",
        "    \"frame_20.jpg\": \"person_2\",\n",
        "\n",
        "    \"frame_67.jpg\": \"person_28\",\n",
        "    \"frame_63.jpg\": \"person_23\",\n",
        "\n",
        "    \"frame_76.jpg\": \"person_25\",\n",
        "    \"frame_2.jpg\": \"person_26\",\n",
        "\n",
        "    \"frame_26.jpg\": \"person_34\",\n",
        "    \"frame_16.jpg\": \"person_35\",\n",
        "\n",
        "    \"frame_0.jpg\": \"person_39\",\n",
        "    \"frame_11.jpg\": \"person_43\",\n",
        "\n",
        "    \"frame_10.jpg\": \"person_48\",\n",
        "    \"frame_52.jpg\": \"person_50\",\n",
        "\n",
        "    \"frame_4.jpg\": \"person_53\",\n",
        "\n",
        "    \"frame_43.jpg\": \"person_69\",\n",
        "    \"frame_17.jpg\": \"person_84\",\n",
        "\n",
        "    \"frame_18.jpg\": \"person_85\",\n",
        "    \"frame_33.jpg\": \"person_86\",\n",
        "\n",
        "    \"frame_5.jpg\": \"person_87\",\n",
        "    \"frame_71.jpg\": \"person_95\",\n",
        "\n",
        "    \"frame_21.jpg\": \"person_102\",\n",
        "    \"frame_19.jpg\": \"person_107\",\n",
        "\n",
        "    \"frame_66.jpg\": \"person_109\",\n",
        "    \"frame_36.jpg\": \"person_114\",\n",
        "\n",
        "    \"frame_9.jpg\": \"person_122\",\n",
        "    \"frame_13.jpg\": \"person_125\",\n",
        "\n",
        "    \"frame_1.jpg\": \"person_148\",\n",
        "    \"frame_3.jpg\": \"person_155\",\n",
        "\n",
        "    \"frame_7.jpg\": \"person_174\",\n",
        "\n",
        "}\n",
        "\n",
        "def get_top_matches(query_features, features_dict_2, top_n=10):\n",
        "    distance_scores = []\n",
        "    for person_id, features in features_dict_2.items():\n",
        "        stored_features = np.mean(features, axis=0)  # Average features for consistency\n",
        "        distance = cosine(query_features.flatten(), stored_features.flatten())  # Cosine distance\n",
        "        distance_scores.append((person_id, distance))\n",
        "\n",
        "    distance_scores.sort(key=lambda x: x[1])  # Sort by lowest distance\n",
        "    return [match[0] for match in distance_scores[:top_n]]  # Return top N matching person IDs\n",
        "\n",
        "# Compute Rank-1, Rank-5, and Rank-10 accuracy\n",
        "rank1_correct = 0\n",
        "rank5_correct = 0\n",
        "rank10_correct = 0\n",
        "total_queries = len(query_labels)\n",
        "\n",
        "for query_image_name, true_person_id in query_labels.items():\n",
        "    query_image_path = os.path.join(query_folder_path, query_image_name)\n",
        "    query_features = extract_features(query_image_path, resnet50, transform, device)\n",
        "    top_matches = get_top_matches(query_features, features_dict_2, top_n=10)\n",
        "\n",
        "    if true_person_id == top_matches[0]:\n",
        "        rank1_correct += 1\n",
        "    if true_person_id in top_matches[:5]:\n",
        "        rank5_correct += 1\n",
        "    if true_person_id in top_matches[:10]:\n",
        "        rank10_correct += 1\n",
        "\n",
        "rank1_accuracy = rank1_correct / total_queries\n",
        "rank5_accuracy = rank5_correct / total_queries\n",
        "rank10_accuracy = rank10_correct / total_queries\n",
        "\n",
        "print(f\"Rank-1 Accuracy: {rank1_accuracy:.2%}\")\n",
        "print(f\"Rank-5 Accuracy: {rank5_accuracy:.2%}\")\n",
        "print(f\"Rank-10 Accuracy: {rank10_accuracy:.2%}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "csDMF2C2kIzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_average_precision(true_id, retrieved_ids):\n",
        "    \"\"\"Computes the Average Precision (AP) for a single query.\"\"\"\n",
        "    relevant = [1 if str(pid) == str(true_id) else 0 for pid in retrieved_ids]\n",
        "\n",
        "    num_relevant = sum(relevant)\n",
        "    if num_relevant == 0:\n",
        "        return 0.0\n",
        "\n",
        "    cum_sum = 0.0\n",
        "    relevant_count = 0\n",
        "    for i, rel in enumerate(relevant):\n",
        "        if rel == 1:\n",
        "            relevant_count += 1\n",
        "            cum_sum += relevant_count / (i + 1)\n",
        "\n",
        "    return cum_sum / num_relevant\n",
        "\n",
        "def compute_map(query_folder_path, query_labels, features_dict, model, transform, device, top_n=10):\n",
        "    \"\"\"Computes Mean Average Precision (mAP) over all queries.\"\"\"\n",
        "    average_precisions = []\n",
        "\n",
        "    for query_image_name, true_person_id in query_labels.items():\n",
        "        query_image_path = os.path.join(query_folder_path, query_image_name)\n",
        "        query_features = extract_features(query_image_path, resnet50, transform, device)\n",
        "\n",
        "        top_matches = get_top_matches(query_features, features_dict_2, top_n=top_n)\n",
        "\n",
        "        ap = compute_average_precision(true_person_id, top_matches)\n",
        "        average_precisions.append(ap)\n",
        "\n",
        "    mean_ap = np.mean(average_precisions) if average_precisions else 0.0\n",
        "    print(f\"Mean Average Precision (mAP): {mean_ap:.2%}\")\n",
        "    return mean_ap\n",
        "\n",
        "\n",
        "query_labels = {\n",
        "    \"frame_45.jpg\": \"person_1\",\n",
        "    \"frame_20.jpg\": \"person_2\",\n",
        "    \"frame_67.jpg\": \"person_28\",\n",
        "    \"frame_63.jpg\": \"person_23\",\n",
        "    \"frame_76.jpg\": \"person_25\",\n",
        "    \"frame_2.jpg\": \"person_26\",\n",
        "    \"frame_26.jpg\": \"person_34\",\n",
        "    \"frame_16.jpg\": \"person_35\",\n",
        "    \"frame_0.jpg\": \"person_39\",\n",
        "    \"frame_11.jpg\": \"person_43\",\n",
        "    \"frame_10.jpg\": \"person_48\",\n",
        "    \"frame_52.jpg\": \"person_50\",\n",
        "    \"frame_4.jpg\": \"person_53\",\n",
        "    \"frame_43.jpg\": \"person_69\",\n",
        "    \"frame_17.jpg\": \"person_84\",\n",
        "    \"frame_18.jpg\": \"person_85\",\n",
        "    \"frame_33.jpg\": \"person_86\",\n",
        "    \"frame_5.jpg\": \"person_87\",\n",
        "    \"frame_71.jpg\": \"person_95\",\n",
        "    \"frame_21.jpg\": \"person_102\",\n",
        "    \"frame_19.jpg\": \"person_107\",\n",
        "    \"frame_66.jpg\": \"person_109\",\n",
        "    \"frame_36.jpg\": \"person_114\",\n",
        "    \"frame_9.jpg\": \"person_122\",\n",
        "    \"frame_13.jpg\": \"person_125\",\n",
        "    \"frame_1.jpg\": \"person_148\",\n",
        "    \"frame_3.jpg\": \"person_155\",\n",
        "    \"frame_7.jpg\": \"person_174\",\n",
        "}\n",
        "\n",
        "mean_ap = compute_map(\"query_images\", query_labels, features_dict_2, resnet50, transform, device)\n"
      ],
      "metadata": {
        "id": "EjCAzbBlk86C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy values\n",
        "rank1_acc = 60.71\n",
        "rank5_acc = 78.57\n",
        "rank10_acc = 92.86\n",
        "\n",
        "# Ranks\n",
        "ranks = [1, 5, 10]\n",
        "accuracies = [rank1_acc, rank5_acc, rank10_acc]\n",
        "\n",
        "# Plot accuracy trends\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(ranks, accuracies, marker='o', linestyle='-', color='b', label='Accuracy')\n",
        "plt.xlabel('Rank')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Rank-wise Accuracy Curve')\n",
        "plt.xticks(ranks)\n",
        "plt.yticks(range(50, 101, 5))\n",
        "plt.ylim(50, 105)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "C-_veDOdp-Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create the dataframe and show rank1,rank5,rank10 and map score of swin transformer,resnet50 and vgg19 models\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'Model': ['Swin Transformer', 'ResNet50'],\n",
        "    'Rank 1': [92.86, 60.71],\n",
        "    'Rank 5': [100.00, 78.57],\n",
        "    'Rank 10': [100.00, 92.86],\n",
        "    'mAP Score': [95.83, 67.24]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df\n"
      ],
      "metadata": {
        "id": "dGu2YBgI_Tnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "data = {\n",
        "    'Model': ['Swin Transformer', 'ResNet50'],\n",
        "    'Rank 1': [92.86, 60.71],\n",
        "    'Rank 5': [100.00, 78.57],\n",
        "    'Rank 10': [100.00, 92.86],\n",
        "    'mAP Score': [95.83,67.2]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Plotting the data\n",
        "models = df['Model']\n",
        "rank1 = df['Rank 1']\n",
        "rank5 = df['Rank 5']\n",
        "rank10 = df['Rank 10']\n",
        "map_scores = df['mAP Score']\n",
        "\n",
        "x = range(len(models))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.plot(x, rank1, marker='o', label='Rank 1')\n",
        "plt.plot(x, rank5, marker='s', label='Rank 5')\n",
        "plt.plot(x, rank10, marker='^', label='Rank 10')\n",
        "plt.plot(x, map_scores, marker='x', label='mAP Score')\n",
        "\n",
        "plt.xticks(x, models, rotation=45, ha='right')  # Rotate model names for better readability\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Accuracy (%) / mAP Score')\n",
        "plt.title('Rank-wise Accuracy and mAP Score Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()  # Adjust layout to prevent labels from overlapping\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x0-HolST_sjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EKC3vzdeufFE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}